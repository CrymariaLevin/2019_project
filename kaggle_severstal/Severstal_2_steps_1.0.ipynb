{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/data/Kaggle/Severstal/tta_wrapper-master/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50272, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ClassId</th>\n",
       "      <th>hasMask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00031f466.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels  \\\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...   \n",
       "1  0002cc93b.jpg_2                                                NaN   \n",
       "2  0002cc93b.jpg_3                                                NaN   \n",
       "3  0002cc93b.jpg_4                                                NaN   \n",
       "4  00031f466.jpg_1                                                NaN   \n",
       "\n",
       "         ImageId ClassId  hasMask  \n",
       "0  0002cc93b.jpg       1     True  \n",
       "1  0002cc93b.jpg       2    False  \n",
       "2  0002cc93b.jpg       3    False  \n",
       "3  0002cc93b.jpg       4    False  \n",
       "4  00031f466.jpg       1    False  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('D:/data/Kaggle/Severstal/train.csv')\n",
    "train_df['ImageId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "train_df['ClassId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[1])\n",
    "train_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12568, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>hasMask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10803</th>\n",
       "      <td>db4867ee8.jpg</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11776</th>\n",
       "      <td>ef24da2ba.jpg</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6284</th>\n",
       "      <td>7f30b9c64.jpg</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>bf0c81db6.jpg</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9615</th>\n",
       "      <td>c314f43f3.jpg</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ImageId  hasMask\n",
       "10803  db4867ee8.jpg      3.0\n",
       "11776  ef24da2ba.jpg      3.0\n",
       "6284   7f30b9c64.jpg      2.0\n",
       "9421   bf0c81db6.jpg      2.0\n",
       "9615   c314f43f3.jpg      2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mask_count_df = train_df.groupby('ImageId').agg(np.sum).reset_index()\n",
    "mask_count_df.sort_values('hasMask', ascending=False, inplace=True)\n",
    "print(mask_count_df.shape)\n",
    "mask_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>ImageId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004f40c73.jpg_1</td>\n",
       "      <td>1 1</td>\n",
       "      <td>004f40c73.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004f40c73.jpg_2</td>\n",
       "      <td>1 1</td>\n",
       "      <td>004f40c73.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004f40c73.jpg_3</td>\n",
       "      <td>1 1</td>\n",
       "      <td>004f40c73.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004f40c73.jpg_4</td>\n",
       "      <td>1 1</td>\n",
       "      <td>004f40c73.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006f39c41.jpg_1</td>\n",
       "      <td>1 1</td>\n",
       "      <td>006f39c41.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId EncodedPixels        ImageId\n",
       "0  004f40c73.jpg_1           1 1  004f40c73.jpg\n",
       "1  004f40c73.jpg_2           1 1  004f40c73.jpg\n",
       "2  004f40c73.jpg_3           1 1  004f40c73.jpg\n",
       "3  004f40c73.jpg_4           1 1  004f40c73.jpg\n",
       "4  006f39c41.jpg_1           1 1  006f39c41.jpg"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test测试集\n",
    "import pandas\n",
    "\n",
    "sub_df = pandas.read_csv('D:/data/Kaggle/Severstal/sample_submission.csv')\n",
    "sub_df['ImageId'] = sub_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004f40c73.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>006f39c41.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00b7fb703.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00bbcd9af.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0108ce457.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId\n",
       "0  004f40c73.jpg\n",
       "1  006f39c41.jpg\n",
       "2  00b7fb703.jpg\n",
       "3  00bbcd9af.jpg\n",
       "4  0108ce457.jpg"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs = pandas.DataFrame(sub_df['ImageId'].unique(), columns=['ImageId'])#只根据图片名称建表\n",
    "print(len(test_imgs))\n",
    "test_imgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>hasMask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10803</th>\n",
       "      <td>db4867ee8.jpg</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11776</th>\n",
       "      <td>ef24da2ba.jpg</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6284</th>\n",
       "      <td>7f30b9c64.jpg</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>bf0c81db6.jpg</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9615</th>\n",
       "      <td>c314f43f3.jpg</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ImageId  hasMask\n",
       "10803  db4867ee8.jpg      3.0\n",
       "11776  ef24da2ba.jpg      3.0\n",
       "6284   7f30b9c64.jpg      2.0\n",
       "9421   bf0c81db6.jpg      2.0\n",
       "9615   c314f43f3.jpg      2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_missing_train_idx = mask_count_df[mask_count_df['hasMask'] > 0]\n",
    "non_missing_train_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_img(code, base, resize=True):\n",
    "    path = f'{base}/{code}'\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if resize:\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def validate_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1801/1801 [00:15<00:00, 118.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_path = 'D:/data/Kaggle/Severstal/tmp/test'\n",
    "validate_path(test_path)\n",
    "\n",
    "for code in tqdm(test_imgs['ImageId'].values):\n",
    "    img = load_img(code, base=r'D:\\data\\Kaggle\\Severstal/test_imgs')\n",
    "    path = code.replace('.jpg', '')\n",
    "    cv2.imwrite(f'{test_path}/{path}.png', img)\n",
    "\n",
    "test_imgs['ImageId'] = test_imgs['ImageId'].apply(lambda x: x.replace('.jpg', '.png'))\n",
    "sub_df['ImageId'] = sub_df['ImageId'].apply(lambda x: x.replace('.jpg', '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1801 images.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1/255.).flow_from_dataframe(\n",
    "        test_imgs,\n",
    "        directory=test_path, #目标目录的路径，如果写了要包含在 dataframe 中映射的所有图像\n",
    "        x_col='ImageId',\n",
    "        class_mode=None,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False #不打乱顺序\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Model)          (None, 8, 8, 1024)        7037504   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 7,568,961\n",
      "Trainable params: 7,482,241\n",
      "Non-trainable params: 86,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "\n",
    "remove_model = load_model('D:/data/Kaggle/Severstal/pre_model.h5')\n",
    "remove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>allMissing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004f40c73.png</td>\n",
       "      <td>0.914483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>006f39c41.png</td>\n",
       "      <td>0.999879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00b7fb703.png</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00bbcd9af.png</td>\n",
       "      <td>0.996996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0108ce457.png</td>\n",
       "      <td>0.999664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId  allMissing\n",
       "0  004f40c73.png    0.914483\n",
       "1  006f39c41.png    0.999879\n",
       "2  00b7fb703.png    0.999982\n",
       "3  00bbcd9af.png    0.996996\n",
       "4  0108ce457.png    0.999664"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_missing_pred = remove_model.predict_generator(\n",
    "    test_gen,\n",
    "    steps=len(test_gen),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#test_imgs['missingCount'] = test_missing_pred.argmax(axis=1) + 1\n",
    "test_imgs['allMissing'] = test_missing_pred #allMissing列值等于1代表没有给定的缺陷类型，即无缺陷\n",
    "test_imgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1801\n",
       "Name: missingCount, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs['missingCount'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(770, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>allMissing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0109b68ec.png</td>\n",
       "      <td>0.070452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>010ec96b4.png</td>\n",
       "      <td>0.002799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>017bd7ce3.png</td>\n",
       "      <td>0.022317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>01d49cd47.png</td>\n",
       "      <td>0.002857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>020ffb2d3.png</td>\n",
       "      <td>0.058497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ImageId  allMissing\n",
       "5   0109b68ec.png    0.070452\n",
       "6   010ec96b4.png    0.002799\n",
       "8   017bd7ce3.png    0.022317\n",
       "10  01d49cd47.png    0.002857\n",
       "11  020ffb2d3.png    0.058497"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_test_imgs = test_imgs[test_imgs['allMissing'] < 0.5]\n",
    "print(filtered_test_imgs.shape)\n",
    "filtered_test_imgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Selvaria\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3080, 3)\n",
      "(4124, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>ImageId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0109b68ec.jpg_1</td>\n",
       "      <td>1 1</td>\n",
       "      <td>0109b68ec.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0109b68ec.jpg_2</td>\n",
       "      <td>1 1</td>\n",
       "      <td>0109b68ec.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0109b68ec.jpg_3</td>\n",
       "      <td>1 1</td>\n",
       "      <td>0109b68ec.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0109b68ec.jpg_4</td>\n",
       "      <td>1 1</td>\n",
       "      <td>0109b68ec.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010ec96b4.jpg_1</td>\n",
       "      <td>1 1</td>\n",
       "      <td>010ec96b4.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId EncodedPixels        ImageId\n",
       "0  0109b68ec.jpg_1           1 1  0109b68ec.jpg\n",
       "1  0109b68ec.jpg_2           1 1  0109b68ec.jpg\n",
       "2  0109b68ec.jpg_3           1 1  0109b68ec.jpg\n",
       "3  0109b68ec.jpg_4           1 1  0109b68ec.jpg\n",
       "4  010ec96b4.jpg_1           1 1  010ec96b4.jpg"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_mask = sub_df['ImageId'].isin(filtered_test_imgs[\"ImageId\"].values)\n",
    "filtered_sub_df = sub_df[filtered_mask].copy()\n",
    "\n",
    "null_sub_df = sub_df[~filtered_mask].copy()\n",
    "null_sub_df['EncodedPixels'] = null_sub_df['EncodedPixels'].apply(lambda x: ' ')\n",
    "\n",
    "filtered_sub_df['ImageId'] = filtered_sub_df['ImageId'].apply(\n",
    "    lambda x: x.replace('.png', '.jpg'))\n",
    "filtered_test_imgs['ImageId'] = filtered_test_imgs['ImageId'].apply(\n",
    "    lambda x: x.replace('.png', '.jpg'))\n",
    "\n",
    "filtered_sub_df.reset_index(drop=True, inplace=True)\n",
    "filtered_test_imgs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(filtered_sub_df.shape)\n",
    "print(null_sub_df.shape)\n",
    "\n",
    "filtered_sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29102 12 29346 24 29602 24 29858 24 30114 24 30370 24 30626 24 30882 24 31139 23 31395 23 31651 23 31907 23 32163 23 32419 23 32675 23 77918 27 78174 55 78429 60 78685 64 78941 68 79197 72 79452 77 79708 81 79964 85 80220 89 80475 94 80731 98 80987 102 81242 105 81498 105 81754 104 82010 104 82265 105 82521 31 82556 69 82779 27 82818 63 83038 22 83080 57 83297 17 83342 50 83555 13 83604 44 83814 8 83866 37 84073 3 84128 31 84390 25 84652 18 84918 8 85239 10 85476 29 85714 47 85960 57 86216 57 86471 58 86727 58 86983 58 87238 59 87494 59 87750 59 88005 60 88261 60 88517 60 88772 61 89028 53 89283 40 89539 32 89667 10 89795 30 89923 28 90050 29 90179 37 90306 27 90434 38 90562 14 90690 38 90817 9 90946 38 91073 3 91202 38 91458 38 91714 38 91969 39 92225 39 92481 39 92737 39 92993 39 93248 40 93504 40 93760 40 94026 30 94302 10 189792 7 190034 21 190283 28 190539 28 190795 28 191051 28 191307 28 191563 28 191819 28 192075 28 192331 28 192587 28 192843 23 193099 14 193355 5\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] (256, 1600)\n",
      "29102 12 29346 24 29602 24 29858 24 30114 24 30370 24 30626 24 30882 24 31139 23 31395 23 31651 23 31907 23 32163 23 32419 23 32675 23 77918 27 78174 55 78429 60 78685 64 78941 68 79197 72 79452 77 79708 81 79964 85 80220 89 80475 94 80731 98 80987 102 81242 105 81498 105 81754 104 82010 104 82265 105 82521 31 82556 69 82779 27 82818 63 83038 22 83080 57 83297 17 83342 50 83555 13 83604 44 83814 8 83866 37 84073 3 84128 31 84390 25 84652 18 84918 8 85239 10 85476 29 85714 47 85960 57 86216 57 86471 58 86727 58 86983 58 87238 59 87494 59 87750 59 88005 60 88261 60 88517 60 88772 61 89028 53 89283 40 89539 32 89667 10 89795 30 89923 28 90050 29 90179 37 90306 27 90434 38 90562 14 90690 38 90817 9 90946 38 91073 3 91202 38 91458 38 91714 38 91969 39 92225 39 92481 39 92737 39 92993 39 93248 40 93504 40 93760 40 94026 30 94302 10 189792 7 190034 21 190283 28 190539 28 190795 28 191051 28 191307 28 191563 28 191819 28 192075 28 192331 28 192587 28 192843 23 193099 14 193355 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle2mask(rle, input_shape):\n",
    "    width, height = input_shape[:2]\n",
    "    \n",
    "    mask= np.zeros( width*height ).astype(np.uint8)\n",
    "    \n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "    \n",
    "    starts = starts - 1\n",
    "\n",
    "    for index, start in enumerate(starts):\n",
    "        mask[int(start):int(start+lengths[index])] = 1\n",
    "        \n",
    "    return mask.reshape(height, width).T\n",
    "\n",
    "def build_masks(rles, input_shape):\n",
    "    depth = len(rles)\n",
    "    masks = np.zeros((*input_shape, depth))\n",
    "    \n",
    "    for i, rle in enumerate(rles):\n",
    "        if type(rle) is str:\n",
    "            masks[:, :, i] = rle2mask(rle, input_shape)\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def build_rles(masks):\n",
    "    width, height, depth = masks.shape\n",
    "    \n",
    "    rles = [mask2rle(masks[:, :, i])\n",
    "            for i in range(depth)]\n",
    "    \n",
    "    return rles\n",
    "\n",
    "print(train_df.loc[0,'EncodedPixels'])\n",
    "mask_tensor = rle2mask(train_df.loc[0,'EncodedPixels'], (256,1600,1))\n",
    "print(mask_tensor, mask_tensor.shape)\n",
    "print(mask2rle(rle2mask(train_df.loc[0,'EncodedPixels'], (256,1600,1))))\n",
    "mask2rle(rle2mask('1 1',((256,1600,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import gray2rgb\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n",
    "                 base_path='D:/data/Kaggle/Severstal/train_imgs',\n",
    "                 batch_size=32, dim=(256, 1600), n_channels=3,\n",
    "                 n_classes=4, random_state=2019, shuffle=True,aug=False):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df\n",
    "        self.mode = mode\n",
    "        self.base_path = base_path\n",
    "        self.target_df = target_df\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.seq = aug\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n",
    "                \n",
    "        X = self.__generate_X(list_IDs_batch)\n",
    "        if self.mode == 'fit':\n",
    "            y = self.__generate_y(list_IDs_batch)\n",
    "            return X, y.astype(int)\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            return X\n",
    "\n",
    "        else:\n",
    "            raise AttributeError('The mode parameter should be set to \"fit\" or \"predict\".')\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.seed(self.random_state)\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __generate_X(self, list_IDs_batch):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            im_name = self.df['ImageId'].iloc[ID]\n",
    "            img_path = f\"{self.base_path}/{im_name}\"\n",
    "            img = self.__load_grayscale(img_path)\n",
    "            \n",
    "            # Store samples\n",
    "            X[i,] = gray2rgb(img[:,:,0])\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def __generate_y(self, list_IDs_batch):\n",
    "        y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            im_name = self.df['ImageId'].iloc[ID]\n",
    "            image_df = self.target_df[self.target_df['ImageId'] == im_name]\n",
    "            \n",
    "            rles = image_df['EncodedPixels'].values\n",
    "            masks = build_masks(rles, input_shape=self.dim)\n",
    "            \n",
    "            y[i, ] = masks\n",
    "        return y\n",
    "    \n",
    "    def __load_grayscale(self, img_path):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def __load_rgb(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Sharpen((0.0, 1.0)),       # sharpen the image\n",
    "    iaa.Fliplr(),\n",
    "    iaa.Flipud(),\n",
    "    iaa.ElasticTransformation(alpha=50, sigma=5)  # apply water effect (affects segmaps)\n",
    "], random_order=True)\n",
    "\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    non_missing_train_idx.index,  # NOTICE DIFFERENCE\n",
    "    random_state=444, \n",
    "    test_size=0.15\n",
    ")\n",
    "\n",
    "train_generator = DataGenerator(\n",
    "    train_idx, \n",
    "    df=mask_count_df,\n",
    "    target_df=train_df,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    n_classes=4,\n",
    "    aug = seq\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    val_idx, \n",
    "    df=mask_count_df,\n",
    "    target_df=train_df,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    n_classes=4,\n",
    "    aug = seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"lines.markerfacecolor\" on line 11 in\n",
      "D:\\Selvaria\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"lines.markeredgecolor\" on line 12 in\n",
      "D:\\Selvaria\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'keymap.help'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e9f8742ca01d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"InlineBackend.figure_format = 'svg'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2093\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2094\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2095\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2096\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-108>\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   2976\u001b[0m                 \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2978\u001b[1;33m         \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivate_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2979\u001b[0m         \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigure_inline_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mactivate_matplotlib\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'backend'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdedent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_deprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend_bases\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFigureCanvasBase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigaspect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m from matplotlib import (\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mbackend_tools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtight_bbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     widgets, get_backend, is_interactive, rcParams)\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_tools.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mToolHelpBase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mToolBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Print tool list, shortcuts and description'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[0mdefault_keymap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keymap.help'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_tools.py\u001b[0m in \u001b[0;36mToolHelpBase\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mToolHelpBase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mToolBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Print tool list, shortcuts and description'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m     \u001b[0mdefault_keymap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keymap.help'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'help.png'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    948\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m  \u001b[1;31m# try to convert to proper type or raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m  \u001b[1;31m# try to convert to proper type or skip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'keymap.help'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i=0\n",
    "plt.imshow(y[i].sum(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import  ModelCheckpoint\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils import conv_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras import backend as K\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import Callback\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import multiply\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.legacy import interfaces\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.core import Activation, SpatialDropout2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.core import Dense, Lambda\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if activation == True:\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16):\n",
    "    x = LeakyReLU(alpha=0.1)(blockInput)\n",
    "    x = BatchNormalization()(x)\n",
    "    blockInput = BatchNormalization()(blockInput)\n",
    "    x = convolution_block(x, num_filters, (3,3) )\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet as efn \n",
    "\n",
    "def UEfficientNet(input_shape=(None, None, 3),dropout_rate=0.1):\n",
    "\n",
    "    backbone = efn.EfficientNetB2(weights=None,\n",
    "                            include_top=False,\n",
    "                            input_shape=input_shape)\n",
    "#     backbone.load_weights(\"../input/efficientnet-keras-weights-b0b5/efficientnet-b2_imagenet_1000_notop.h5\")\n",
    "    inputs = backbone.input\n",
    "    start_neurons = 8\n",
    "    \n",
    "    i=2\n",
    "    lr = []\n",
    "    for l in backbone.layers:\n",
    "        if l.name == 'block{}a_expand_activation'.format(i):\n",
    "            lr.append(l)\n",
    "            i+=1\n",
    "\n",
    "    print(len(lr))\n",
    "    conv4 = lr[-1].output\n",
    "    conv4 = LeakyReLU(alpha=0.1)(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(dropout_rate)(pool4)\n",
    "    \n",
    "     # Middle\n",
    "    convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\")(pool4)\n",
    "    convm = residual_block(convm,start_neurons * 32)\n",
    "    convm = residual_block(convm,start_neurons * 32)\n",
    "    convm = LeakyReLU(alpha=0.1)(convm)\n",
    "    \n",
    "    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    deconv4_up1 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(deconv4)\n",
    "    deconv4_up2 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(deconv4_up1)\n",
    "    deconv4_up3 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(deconv4_up2)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(dropout_rate)(uconv4) \n",
    "    \n",
    "    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 16)\n",
    "#     uconv4 = residual_block(uconv4,start_neurons * 16)\n",
    "    uconv4 = LeakyReLU(alpha=0.1)(uconv4)  #conv1_2\n",
    "    \n",
    "    deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    deconv3_up1 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(deconv3)\n",
    "    deconv3_up2 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(deconv3_up1)\n",
    "    conv3 = lr[-2].output\n",
    "    uconv3 = concatenate([deconv3,deconv4_up1, conv3])    \n",
    "    uconv3 = Dropout(dropout_rate)(uconv3)\n",
    "    \n",
    "    uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 8)\n",
    "#     uconv3 = residual_block(uconv3,start_neurons * 8)\n",
    "    uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    deconv2_up1 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(deconv2)\n",
    "    conv2 = lr[-4].output\n",
    "    uconv2 = concatenate([deconv2,deconv3_up1,deconv4_up2, conv2])\n",
    "        \n",
    "    uconv2 = Dropout(0.1)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 4)\n",
    "#     uconv2 = residual_block(uconv2,start_neurons * 4)\n",
    "    uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n",
    "    \n",
    "    deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    conv1 = lr[-5].output\n",
    "    uconv1 = concatenate([deconv1,deconv2_up1,deconv3_up2,deconv4_up3, conv1])\n",
    "    \n",
    "    uconv1 = Dropout(0.1)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 2)\n",
    "#     uconv1 = residual_block(uconv1,start_neurons * 2)\n",
    "    uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n",
    "    \n",
    "    uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv1)   \n",
    "    uconv0 = Dropout(0.1)(uconv0)\n",
    "    uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n",
    "    uconv0 = residual_block(uconv0,start_neurons * 1)\n",
    "#     uconv0 = residual_block(uconv0,start_neurons * 1)\n",
    "    uconv0 = LeakyReLU(alpha=0.1)(uconv0)\n",
    "    \n",
    "    uconv0 = Dropout(dropout_rate/2)(uconv0)\n",
    "    uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv0) \n",
    "    output_layer = Conv2D(4, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n",
    "    \n",
    "    model = Model(inputs, output_layer)\n",
    "    model.name = 'u-xception'\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-4588ad6f4b9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUEfficientNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1600\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-97b43088e906>\u001b[0m in \u001b[0;36mUEfficientNet\u001b[1;34m(input_shape, dropout_rate)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mconv4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mconv4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mpool4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model = UEfficientNet(input_shape=(256,1600,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 256, 1600, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_507 (Conv2D)             (None, 256, 1600, 8) 224         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_508 (Conv2D)             (None, 256, 1600, 8) 584         conv2d_507[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 128, 800, 8)  0           conv2d_508[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_509 (Conv2D)             (None, 128, 800, 16) 1168        max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_510 (Conv2D)             (None, 128, 800, 16) 2320        conv2d_509[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 64, 400, 16)  0           conv2d_510[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_511 (Conv2D)             (None, 64, 400, 32)  4640        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_512 (Conv2D)             (None, 64, 400, 32)  9248        conv2d_511[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 32, 200, 32)  0           conv2d_512[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_513 (Conv2D)             (None, 32, 200, 64)  18496       max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_514 (Conv2D)             (None, 32, 200, 64)  36928       conv2d_513[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 16, 100, 64)  0           conv2d_514[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_515 (Conv2D)             (None, 16, 100, 64)  36928       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_516 (Conv2D)             (None, 16, 100, 64)  36928       conv2d_515[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 8, 50, 64)    0           conv2d_516[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_517 (Conv2D)             (None, 8, 50, 128)   73856       max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_518 (Conv2D)             (None, 8, 50, 128)   147584      conv2d_517[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 16, 100, 64)  32832       conv2d_518[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 100, 128) 0           conv2d_transpose_11[0][0]        \n",
      "                                                                 conv2d_516[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_519 (Conv2D)             (None, 16, 100, 64)  73792       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_520 (Conv2D)             (None, 16, 100, 64)  36928       conv2d_519[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, 32, 200, 32)  8224        conv2d_520[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 200, 96)  0           conv2d_transpose_12[0][0]        \n",
      "                                                                 conv2d_514[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_521 (Conv2D)             (None, 32, 200, 32)  27680       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_522 (Conv2D)             (None, 32, 200, 32)  9248        conv2d_521[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DTran (None, 64, 400, 32)  4128        conv2d_522[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64, 400, 64)  0           conv2d_transpose_13[0][0]        \n",
      "                                                                 conv2d_512[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_523 (Conv2D)             (None, 64, 400, 32)  18464       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_524 (Conv2D)             (None, 64, 400, 32)  9248        conv2d_523[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DTran (None, 128, 800, 16) 2064        conv2d_524[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 128, 800, 32) 0           conv2d_transpose_14[0][0]        \n",
      "                                                                 conv2d_510[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_525 (Conv2D)             (None, 128, 800, 16) 4624        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_526 (Conv2D)             (None, 128, 800, 16) 2320        conv2d_525[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DTran (None, 256, 1600, 8) 520         conv2d_526[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 256, 1600, 16 0           conv2d_transpose_15[0][0]        \n",
      "                                                                 conv2d_508[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_527 (Conv2D)             (None, 256, 1600, 8) 1160        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_528 (Conv2D)             (None, 256, 1600, 8) 584         conv2d_527[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_529 (Conv2D)             (None, 256, 1600, 4) 36          conv2d_528[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 600,756\n",
      "Trainable params: 600,756\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "inputs = Input((256, 1600, 3)) #原图尺寸，黑白单通道\n",
    "\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (inputs)\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(64, (3, 3), activation='relu', padding='same') (p4)\n",
    "c5 = Conv2D(64, (3, 3), activation='relu', padding='same') (c5)\n",
    "p5 = MaxPooling2D(pool_size=(2, 2)) (c5) #到这里一直是在对原始的input进行层操作变换\n",
    "\n",
    "c55 = Conv2D(128, (3, 3), activation='relu', padding='same') (p5) #开始对原始的input进行变换，直到u6的反向卷积\n",
    "c55 = Conv2D(128, (3, 3), activation='relu', padding='same') (c55)\n",
    "\n",
    "#应该是使用了残差连接，用于避免梯度消失和表示瓶颈\n",
    "u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c55) #转置卷积层 (有时被成为反卷积)，将具有该卷积层输出shape的tensor转换为具有该卷积层输入shape的tensor。同时保留与卷积层兼容的连接模式\n",
    "u6 = concatenate([u6, c5]) #和上一步在一起，相当于降维后连结在一起，和之前的维度相同。默认axis=-1(最后一个轴)，相当于按照c5[-1轴],u6[-1轴]拼接，即\n",
    "c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "u71 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u71 = concatenate([u71, c4])\n",
    "c71 = Conv2D(32, (3, 3), activation='relu', padding='same') (u71)\n",
    "c61 = Conv2D(32, (3, 3), activation='relu', padding='same') (c71)\n",
    "\n",
    "u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c61)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "outputs = Conv2D(4, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])#使用之前写的评分函数\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/708 [======>.......................] - ETA: 5:26:31 - loss: 1.6773 - dice_coef: 7.5411e- - ETA: 3:24:29 - loss: 1.6821 - dice_coef: 3.8764e- - ETA: 2:43:31 - loss: 1.6818 - dice_coef: 2.6343e- - ETA: 2:23:20 - loss: 1.6805 - dice_coef: 0.0011   - ETA: 2:10:35 - loss: 1.6792 - dice_coef: 9.1534e- - ETA: 2:02:10 - loss: 1.6752 - dice_coef: 7.6279e- - ETA: 1:56:13 - loss: 1.6716 - dice_coef: 0.0013   - ETA: 1:52:04 - loss: 1.6687 - dice_coef: 0.00 - ETA: 1:48:36 - loss: 1.6635 - dice_coef: 0.00 - ETA: 1:45:43 - loss: 1.6591 - dice_coef: 9.0536e- - ETA: 1:43:23 - loss: 1.6517 - dice_coef: 8.2305e- - ETA: 1:41:27 - loss: 1.6451 - dice_coef: 0.0017   - ETA: 1:39:43 - loss: 1.6362 - dice_coef: 0.00 - ETA: 1:38:20 - loss: 1.6351 - dice_coef: 0.00 - ETA: 1:37:23 - loss: 1.6296 - dice_coef: 0.00 - ETA: 1:36:12 - loss: 1.6233 - dice_coef: 0.00 - ETA: 1:35:16 - loss: 1.6170 - dice_coef: 0.00 - ETA: 1:34:27 - loss: 1.6120 - dice_coef: 0.00 - ETA: 1:33:32 - loss: 1.6070 - dice_coef: 0.00 - ETA: 1:32:42 - loss: 1.6028 - dice_coef: 0.00 - ETA: 1:32:07 - loss: 1.5986 - dice_coef: 0.00 - ETA: 1:31:34 - loss: 1.5942 - dice_coef: 0.00 - ETA: 1:30:55 - loss: 1.5900 - dice_coef: 0.00 - ETA: 1:30:19 - loss: 1.5845 - dice_coef: 0.00 - ETA: 1:29:52 - loss: 1.5801 - dice_coef: 0.00 - ETA: 1:29:24 - loss: 1.5748 - dice_coef: 0.00 - ETA: 1:29:00 - loss: 1.5709 - dice_coef: 0.00 - ETA: 1:28:36 - loss: 1.5670 - dice_coef: 0.00 - ETA: 1:28:22 - loss: 1.5628 - dice_coef: 0.00 - ETA: 1:27:56 - loss: 1.5584 - dice_coef: 0.00 - ETA: 1:27:31 - loss: 1.5530 - dice_coef: 0.00 - ETA: 1:27:09 - loss: 1.5488 - dice_coef: 0.00 - ETA: 1:26:50 - loss: 1.5445 - dice_coef: 0.00 - ETA: 1:26:32 - loss: 1.5400 - dice_coef: 0.00 - ETA: 1:26:10 - loss: 1.5341 - dice_coef: 0.00 - ETA: 1:25:52 - loss: 1.5296 - dice_coef: 0.00 - ETA: 1:25:35 - loss: 1.5244 - dice_coef: 0.00 - ETA: 1:25:27 - loss: 1.5188 - dice_coef: 0.00 - ETA: 1:25:13 - loss: 1.5129 - dice_coef: 0.00 - ETA: 1:24:56 - loss: 1.5065 - dice_coef: 0.00 - ETA: 1:24:40 - loss: 1.4989 - dice_coef: 0.00 - ETA: 1:24:25 - loss: 1.4900 - dice_coef: 0.00 - ETA: 1:24:08 - loss: 1.4801 - dice_coef: 0.00 - ETA: 1:24:01 - loss: 1.4698 - dice_coef: 0.00 - ETA: 1:23:52 - loss: 1.4605 - dice_coef: 0.00 - ETA: 1:23:38 - loss: 1.4512 - dice_coef: 0.00 - ETA: 1:23:27 - loss: 1.4428 - dice_coef: 0.00 - ETA: 1:23:14 - loss: 1.4340 - dice_coef: 0.00 - ETA: 1:23:01 - loss: 1.4269 - dice_coef: 0.00 - ETA: 1:22:47 - loss: 1.4203 - dice_coef: 0.00 - ETA: 1:22:33 - loss: 1.4135 - dice_coef: 0.00 - ETA: 1:22:20 - loss: 1.4070 - dice_coef: 0.00 - ETA: 1:22:06 - loss: 1.3998 - dice_coef: 0.00 - ETA: 1:21:55 - loss: 1.3931 - dice_coef: 0.00 - ETA: 1:21:41 - loss: 1.3876 - dice_coef: 0.00 - ETA: 1:21:32 - loss: 1.3822 - dice_coef: 0.00 - ETA: 1:21:21 - loss: 1.3760 - dice_coef: 0.00 - ETA: 1:21:18 - loss: 1.3700 - dice_coef: 0.00 - ETA: 1:21:13 - loss: 1.3645 - dice_coef: 0.00 - ETA: 1:21:04 - loss: 1.3592 - dice_coef: 0.00 - ETA: 1:20:52 - loss: 1.3543 - dice_coef: 0.00 - ETA: 1:20:41 - loss: 1.3496 - dice_coef: 0.00 - ETA: 1:20:31 - loss: 1.3451 - dice_coef: 0.00 - ETA: 1:20:22 - loss: 1.3402 - dice_coef: 0.00 - ETA: 1:20:10 - loss: 1.3354 - dice_coef: 0.00 - ETA: 1:20:00 - loss: 1.3307 - dice_coef: 0.00 - ETA: 1:19:49 - loss: 1.3260 - dice_coef: 0.00 - ETA: 1:19:41 - loss: 1.3218 - dice_coef: 0.00 - ETA: 1:19:31 - loss: 1.3175 - dice_coef: 0.00 - ETA: 1:19:21 - loss: 1.3132 - dice_coef: 0.00 - ETA: 1:19:11 - loss: 1.3090 - dice_coef: 0.00 - ETA: 1:19:00 - loss: 1.3052 - dice_coef: 0.00 - ETA: 1:18:49 - loss: 1.3021 - dice_coef: 0.00 - ETA: 1:18:45 - loss: 1.2986 - dice_coef: 0.00 - ETA: 1:18:36 - loss: 1.2950 - dice_coef: 0.00 - ETA: 1:18:28 - loss: 1.2913 - dice_coef: 0.00 - ETA: 1:18:21 - loss: 1.2877 - dice_coef: 0.00 - ETA: 1:18:17 - loss: 1.2844 - dice_coef: 0.00 - ETA: 1:18:12 - loss: 1.2820 - dice_coef: 0.00 - ETA: 1:18:08 - loss: 1.2788 - dice_coef: 0.00 - ETA: 1:17:58 - loss: 1.2761 - dice_coef: 0.00 - ETA: 1:17:48 - loss: 1.2735 - dice_coef: 0.00 - ETA: 1:17:37 - loss: 1.2701 - dice_coef: 0.00 - ETA: 1:17:27 - loss: 1.2676 - dice_coef: 0.00 - ETA: 1:17:18 - loss: 1.2646 - dice_coef: 0.00 - ETA: 1:17:08 - loss: 1.2621 - dice_coef: 0.00 - ETA: 1:16:58 - loss: 1.2591 - dice_coef: 0.00 - ETA: 1:16:49 - loss: 1.2566 - dice_coef: 0.00 - ETA: 1:16:39 - loss: 1.2537 - dice_coef: 0.00 - ETA: 1:16:29 - loss: 1.2511 - dice_coef: 0.00 - ETA: 1:16:18 - loss: 1.2484 - dice_coef: 0.00 - ETA: 1:16:09 - loss: 1.2460 - dice_coef: 0.00 - ETA: 1:16:02 - loss: 1.2437 - dice_coef: 0.00 - ETA: 1:15:57 - loss: 1.2415 - dice_coef: 0.00 - ETA: 1:15:50 - loss: 1.2399 - dice_coef: 0.00 - ETA: 1:15:40 - loss: 1.2373 - dice_coef: 0.00 - ETA: 1:15:29 - loss: 1.2351 - dice_coef: 0.00 - ETA: 1:15:19 - loss: 1.2331 - dice_coef: 0.00 - ETA: 1:15:12 - loss: 1.2312 - dice_coef: 0.00 - ETA: 1:15:06 - loss: 1.2295 - dice_coef: 0.00 - ETA: 1:14:59 - loss: 1.2279 - dice_coef: 0.00 - ETA: 1:14:51 - loss: 1.2255 - dice_coef: 0.00 - ETA: 1:14:42 - loss: 1.2238 - dice_coef: 0.00 - ETA: 1:14:34 - loss: 1.2219 - dice_coef: 0.00 - ETA: 1:14:26 - loss: 1.2198 - dice_coef: 0.00 - ETA: 1:14:26 - loss: 1.2178 - dice_coef: 0.00 - ETA: 1:14:25 - loss: 1.2162 - dice_coef: 0.00 - ETA: 1:14:23 - loss: 1.2143 - dice_coef: 0.00 - ETA: 1:14:22 - loss: 1.2127 - dice_coef: 0.00 - ETA: 1:14:18 - loss: 1.2109 - dice_coef: 0.00 - ETA: 1:14:15 - loss: 1.2092 - dice_coef: 0.00 - ETA: 1:14:11 - loss: 1.2073 - dice_coef: 0.00 - ETA: 1:14:07 - loss: 1.2056 - dice_coef: 0.00 - ETA: 1:14:03 - loss: 1.2039 - dice_coef: 0.00 - ETA: 1:13:58 - loss: 1.2020 - dice_coef: 0.00 - ETA: 1:13:53 - loss: 1.2002 - dice_coef: 0.00 - ETA: 1:13:49 - loss: 1.1988 - dice_coef: 0.00 - ETA: 1:13:46 - loss: 1.1973 - dice_coef: 0.00 - ETA: 1:13:42 - loss: 1.1960 - dice_coef: 0.00 - ETA: 1:13:37 - loss: 1.1946 - dice_coef: 0.00 - ETA: 1:13:33 - loss: 1.1932 - dice_coef: 0.00 - ETA: 1:13:29 - loss: 1.1916 - dice_coef: 0.00 - ETA: 1:13:25 - loss: 1.1901 - dice_coef: 0.00 - ETA: 1:13:20 - loss: 1.1886 - dice_coef: 0.00 - ETA: 1:13:14 - loss: 1.1871 - dice_coef: 0.00 - ETA: 1:13:11 - loss: 1.1857 - dice_coef: 0.00 - ETA: 1:13:06 - loss: 1.1842 - dice_coef: 0.00 - ETA: 1:13:01 - loss: 1.1828 - dice_coef: 0.00 - ETA: 1:12:55 - loss: 1.1814 - dice_coef: 0.00 - ETA: 1:12:46 - loss: 1.1801 - dice_coef: 0.00 - ETA: 1:12:38 - loss: 1.1791 - dice_coef: 0.00 - ETA: 1:12:28 - loss: 1.1779 - dice_coef: 0.00 - ETA: 1:12:19 - loss: 1.1766 - dice_coef: 0.00 - ETA: 1:12:10 - loss: 1.1754 - dice_coef: 0.00 - ETA: 1:12:02 - loss: 1.1745 - dice_coef: 0.00 - ETA: 1:11:52 - loss: 1.1732 - dice_coef: 0.00 - ETA: 1:11:43 - loss: 1.1721 - dice_coef: 0.00 - ETA: 1:11:33 - loss: 1.1710 - dice_coef: 0.00 - ETA: 1:11:24 - loss: 1.1697 - dice_coef: 0.00 - ETA: 1:11:16 - loss: 1.1685 - dice_coef: 0.00 - ETA: 1:11:09 - loss: 1.1674 - dice_coef: 0.00 - ETA: 1:11:01 - loss: 1.1664 - dice_coef: 0.00 - ETA: 1:10:53 - loss: 1.1653 - dice_coef: 0.00 - ETA: 1:10:45 - loss: 1.1644 - dice_coef: 0.00 - ETA: 1:10:35 - loss: 1.1635 - dice_coef: 0.00 - ETA: 1:10:27 - loss: 1.1629 - dice_coef: 0.00 - ETA: 1:10:19 - loss: 1.1620 - dice_coef: 0.00 - ETA: 1:10:10 - loss: 1.1611 - dice_coef: 0.00 - ETA: 1:10:02 - loss: 1.1600 - dice_coef: 0.00 - ETA: 1:09:54 - loss: 1.1591 - dice_coef: 0.00 - ETA: 1:09:44 - loss: 1.1582 - dice_coef: 0.00 - ETA: 1:09:35 - loss: 1.1575 - dice_coef: 0.00 - ETA: 1:09:26 - loss: 1.1565 - dice_coef: 0.00 - ETA: 1:09:17 - loss: 1.1556 - dice_coef: 0.00 - ETA: 1:09:08 - loss: 1.1549 - dice_coef: 0.00 - ETA: 1:08:59 - loss: 1.1542 - dice_coef: 0.00 - ETA: 1:08:50 - loss: 1.1533 - dice_coef: 0.00 - ETA: 1:08:41 - loss: 1.1526 - dice_coef: 0.00 - ETA: 1:08:32 - loss: 1.1517 - dice_coef: 0.00 - ETA: 1:08:23 - loss: 1.1509 - dice_coef: 0.00 - ETA: 1:08:14 - loss: 1.1502 - dice_coef: 0.00 - ETA: 1:08:05 - loss: 1.1495 - dice_coef: 0.00 - ETA: 1:07:57 - loss: 1.1488 - dice_coef: 0.00 - ETA: 1:07:48 - loss: 1.1482 - dice_coef: 0.00 - ETA: 1:07:39 - loss: 1.1473 - dice_coef: 0.00 - ETA: 1:07:30 - loss: 1.1466 - dice_coef: 0.00 - ETA: 1:07:21 - loss: 1.1459 - dice_coef: 0.00 - ETA: 1:07:11 - loss: 1.1451 - dice_coef: 0.00 - ETA: 1:07:02 - loss: 1.1443 - dice_coef: 0.0028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/708 [=============>................] - ETA: 1:06:53 - loss: 1.1436 - dice_coef: 0.00 - ETA: 1:06:44 - loss: 1.1428 - dice_coef: 0.00 - ETA: 1:06:35 - loss: 1.1420 - dice_coef: 0.00 - ETA: 1:06:26 - loss: 1.1412 - dice_coef: 0.00 - ETA: 1:06:17 - loss: 1.1405 - dice_coef: 0.00 - ETA: 1:06:08 - loss: 1.1401 - dice_coef: 0.00 - ETA: 1:05:59 - loss: 1.1392 - dice_coef: 0.00 - ETA: 1:05:50 - loss: 1.1388 - dice_coef: 0.00 - ETA: 1:05:42 - loss: 1.1379 - dice_coef: 0.00 - ETA: 1:05:33 - loss: 1.1372 - dice_coef: 0.00 - ETA: 1:05:24 - loss: 1.1365 - dice_coef: 0.00 - ETA: 1:05:15 - loss: 1.1358 - dice_coef: 0.00 - ETA: 1:05:06 - loss: 1.1350 - dice_coef: 0.00 - ETA: 1:04:58 - loss: 1.1342 - dice_coef: 0.00 - ETA: 1:04:49 - loss: 1.1334 - dice_coef: 0.00 - ETA: 1:04:41 - loss: 1.1327 - dice_coef: 0.00 - ETA: 1:04:32 - loss: 1.1320 - dice_coef: 0.00 - ETA: 1:04:23 - loss: 1.1312 - dice_coef: 0.00 - ETA: 1:04:14 - loss: 1.1308 - dice_coef: 0.00 - ETA: 1:04:06 - loss: 1.1302 - dice_coef: 0.00 - ETA: 1:03:57 - loss: 1.1294 - dice_coef: 0.00 - ETA: 1:03:49 - loss: 1.1285 - dice_coef: 0.00 - ETA: 1:03:40 - loss: 1.1279 - dice_coef: 0.00 - ETA: 1:03:31 - loss: 1.1274 - dice_coef: 0.00 - ETA: 1:03:23 - loss: 1.1267 - dice_coef: 0.00 - ETA: 1:03:14 - loss: 1.1261 - dice_coef: 0.00 - ETA: 1:03:06 - loss: 1.1255 - dice_coef: 0.00 - ETA: 1:02:57 - loss: 1.1251 - dice_coef: 0.00 - ETA: 1:02:49 - loss: 1.1245 - dice_coef: 0.00 - ETA: 1:02:40 - loss: 1.1240 - dice_coef: 0.00 - ETA: 1:02:32 - loss: 1.1236 - dice_coef: 0.00 - ETA: 1:02:23 - loss: 1.1232 - dice_coef: 0.00 - ETA: 1:02:15 - loss: 1.1224 - dice_coef: 0.00 - ETA: 1:02:07 - loss: 1.1219 - dice_coef: 0.00 - ETA: 1:01:58 - loss: 1.1211 - dice_coef: 0.00 - ETA: 1:01:50 - loss: 1.1207 - dice_coef: 0.00 - ETA: 1:01:41 - loss: 1.1198 - dice_coef: 0.00 - ETA: 1:01:33 - loss: 1.1194 - dice_coef: 0.00 - ETA: 1:01:25 - loss: 1.1187 - dice_coef: 0.00 - ETA: 1:01:16 - loss: 1.1179 - dice_coef: 0.00 - ETA: 1:01:08 - loss: 1.1172 - dice_coef: 0.00 - ETA: 1:01:00 - loss: 1.1164 - dice_coef: 0.00 - ETA: 1:00:51 - loss: 1.1158 - dice_coef: 0.00 - ETA: 1:00:43 - loss: 1.1151 - dice_coef: 0.00 - ETA: 1:00:35 - loss: 1.1146 - dice_coef: 0.00 - ETA: 1:00:27 - loss: 1.1142 - dice_coef: 0.00 - ETA: 1:00:19 - loss: 1.1137 - dice_coef: 0.00 - ETA: 1:00:10 - loss: 1.1131 - dice_coef: 0.00 - ETA: 1:00:02 - loss: 1.1126 - dice_coef: 0.00 - ETA: 59:54 - loss: 1.1121 - dice_coef: 0.0023 - ETA: 59:46 - loss: 1.1117 - dice_coef: 0.00 - ETA: 59:38 - loss: 1.1112 - dice_coef: 0.00 - ETA: 59:29 - loss: 1.1104 - dice_coef: 0.00 - ETA: 59:21 - loss: 1.1100 - dice_coef: 0.00 - ETA: 59:13 - loss: 1.1095 - dice_coef: 0.00 - ETA: 59:05 - loss: 1.1090 - dice_coef: 0.00 - ETA: 58:57 - loss: 1.1084 - dice_coef: 0.00 - ETA: 58:48 - loss: 1.1079 - dice_coef: 0.00 - ETA: 58:40 - loss: 1.1074 - dice_coef: 0.00 - ETA: 58:32 - loss: 1.1069 - dice_coef: 0.00 - ETA: 58:24 - loss: 1.1064 - dice_coef: 0.00 - ETA: 58:16 - loss: 1.1059 - dice_coef: 0.00 - ETA: 58:08 - loss: 1.1056 - dice_coef: 0.00 - ETA: 57:59 - loss: 1.1051 - dice_coef: 0.00 - ETA: 57:51 - loss: 1.1045 - dice_coef: 0.00 - ETA: 57:43 - loss: 1.1040 - dice_coef: 0.00 - ETA: 57:35 - loss: 1.1037 - dice_coef: 0.00 - ETA: 57:27 - loss: 1.1033 - dice_coef: 0.00 - ETA: 57:19 - loss: 1.1027 - dice_coef: 0.00 - ETA: 57:11 - loss: 1.1024 - dice_coef: 0.00 - ETA: 57:03 - loss: 1.1020 - dice_coef: 0.00 - ETA: 56:55 - loss: 1.1017 - dice_coef: 0.00 - ETA: 56:47 - loss: 1.1013 - dice_coef: 0.00 - ETA: 56:39 - loss: 1.1009 - dice_coef: 0.00 - ETA: 56:31 - loss: 1.1003 - dice_coef: 0.00 - ETA: 56:23 - loss: 1.0998 - dice_coef: 0.00 - ETA: 56:15 - loss: 1.0996 - dice_coef: 0.00 - ETA: 56:07 - loss: 1.0993 - dice_coef: 0.00 - ETA: 55:59 - loss: 1.0989 - dice_coef: 0.00 - ETA: 55:51 - loss: 1.0986 - dice_coef: 0.00 - ETA: 55:43 - loss: 1.0980 - dice_coef: 0.00 - ETA: 55:35 - loss: 1.0976 - dice_coef: 0.00 - ETA: 55:27 - loss: 1.0970 - dice_coef: 0.00 - ETA: 55:19 - loss: 1.0965 - dice_coef: 0.00 - ETA: 55:11 - loss: 1.0964 - dice_coef: 0.00 - ETA: 55:03 - loss: 1.0956 - dice_coef: 0.00 - ETA: 54:55 - loss: 1.0953 - dice_coef: 0.00 - ETA: 54:47 - loss: 1.0951 - dice_coef: 0.00 - ETA: 54:40 - loss: 1.0948 - dice_coef: 0.00 - ETA: 54:32 - loss: 1.0945 - dice_coef: 0.00 - ETA: 54:24 - loss: 1.0940 - dice_coef: 0.00 - ETA: 54:16 - loss: 1.0937 - dice_coef: 0.00 - ETA: 54:08 - loss: 1.0933 - dice_coef: 0.00 - ETA: 54:00 - loss: 1.0928 - dice_coef: 0.00 - ETA: 53:52 - loss: 1.0926 - dice_coef: 0.00 - ETA: 53:45 - loss: 1.0923 - dice_coef: 0.00 - ETA: 53:37 - loss: 1.0918 - dice_coef: 0.00 - ETA: 53:29 - loss: 1.0916 - dice_coef: 0.00 - ETA: 53:21 - loss: 1.0912 - dice_coef: 0.00 - ETA: 53:14 - loss: 1.0910 - dice_coef: 0.00 - ETA: 53:07 - loss: 1.0905 - dice_coef: 0.00 - ETA: 52:59 - loss: 1.0901 - dice_coef: 0.00 - ETA: 52:51 - loss: 1.0897 - dice_coef: 0.00 - ETA: 52:44 - loss: 1.0892 - dice_coef: 0.00 - ETA: 52:36 - loss: 1.0890 - dice_coef: 0.00 - ETA: 52:28 - loss: 1.0887 - dice_coef: 0.00 - ETA: 52:21 - loss: 1.0879 - dice_coef: 0.00 - ETA: 52:13 - loss: 1.0874 - dice_coef: 0.00 - ETA: 52:05 - loss: 1.0874 - dice_coef: 0.00 - ETA: 51:57 - loss: 1.0869 - dice_coef: 0.00 - ETA: 51:50 - loss: 1.0865 - dice_coef: 0.00 - ETA: 51:42 - loss: 1.0855 - dice_coef: 0.00 - ETA: 51:34 - loss: 1.0852 - dice_coef: 0.00 - ETA: 51:27 - loss: 1.0849 - dice_coef: 0.00 - ETA: 51:19 - loss: 1.0843 - dice_coef: 0.00 - ETA: 51:11 - loss: 1.0837 - dice_coef: 0.00 - ETA: 51:03 - loss: 1.0835 - dice_coef: 0.00 - ETA: 50:56 - loss: 1.0831 - dice_coef: 0.00 - ETA: 50:48 - loss: 1.0824 - dice_coef: 0.00 - ETA: 50:40 - loss: 1.0820 - dice_coef: 0.00 - ETA: 50:33 - loss: 1.0817 - dice_coef: 0.00 - ETA: 50:25 - loss: 1.0814 - dice_coef: 0.00 - ETA: 50:17 - loss: 1.0810 - dice_coef: 0.00 - ETA: 50:10 - loss: 1.0801 - dice_coef: 0.00 - ETA: 50:02 - loss: 1.0793 - dice_coef: 0.00 - ETA: 49:54 - loss: 1.0791 - dice_coef: 0.00 - ETA: 49:47 - loss: 1.0787 - dice_coef: 0.00 - ETA: 49:39 - loss: 1.0786 - dice_coef: 0.00 - ETA: 49:31 - loss: 1.0782 - dice_coef: 0.00 - ETA: 49:24 - loss: 1.0779 - dice_coef: 0.00 - ETA: 49:16 - loss: 1.0777 - dice_coef: 0.00 - ETA: 49:08 - loss: 1.0772 - dice_coef: 0.00 - ETA: 49:01 - loss: 1.0768 - dice_coef: 0.00 - ETA: 48:53 - loss: 1.0765 - dice_coef: 0.00 - ETA: 48:45 - loss: 1.0764 - dice_coef: 0.00 - ETA: 48:38 - loss: 1.0762 - dice_coef: 0.00 - ETA: 48:30 - loss: 1.0755 - dice_coef: 0.00 - ETA: 48:23 - loss: 1.0752 - dice_coef: 0.01 - ETA: 48:15 - loss: 1.0749 - dice_coef: 0.01 - ETA: 48:07 - loss: 1.0743 - dice_coef: 0.01 - ETA: 48:00 - loss: 1.0734 - dice_coef: 0.01 - ETA: 47:52 - loss: 1.0729 - dice_coef: 0.01 - ETA: 47:45 - loss: 1.0729 - dice_coef: 0.01 - ETA: 47:37 - loss: 1.0725 - dice_coef: 0.01 - ETA: 47:29 - loss: 1.0715 - dice_coef: 0.01 - ETA: 47:22 - loss: 1.0711 - dice_coef: 0.01 - ETA: 47:14 - loss: 1.0709 - dice_coef: 0.01 - ETA: 47:07 - loss: 1.0708 - dice_coef: 0.01 - ETA: 46:59 - loss: 1.0702 - dice_coef: 0.01 - ETA: 46:51 - loss: 1.0700 - dice_coef: 0.01 - ETA: 46:44 - loss: 1.0700 - dice_coef: 0.01 - ETA: 46:36 - loss: 1.0699 - dice_coef: 0.01 - ETA: 46:29 - loss: 1.0698 - dice_coef: 0.01 - ETA: 46:21 - loss: 1.0697 - dice_coef: 0.01 - ETA: 46:13 - loss: 1.0692 - dice_coef: 0.01 - ETA: 46:06 - loss: 1.0689 - dice_coef: 0.01 - ETA: 45:58 - loss: 1.0687 - dice_coef: 0.01 - ETA: 45:51 - loss: 1.0685 - dice_coef: 0.01 - ETA: 45:43 - loss: 1.0684 - dice_coef: 0.01 - ETA: 45:36 - loss: 1.0682 - dice_coef: 0.01 - ETA: 45:28 - loss: 1.0681 - dice_coef: 0.01 - ETA: 45:21 - loss: 1.0682 - dice_coef: 0.01 - ETA: 45:13 - loss: 1.0680 - dice_coef: 0.01 - ETA: 45:06 - loss: 1.0678 - dice_coef: 0.01 - ETA: 44:58 - loss: 1.0674 - dice_coef: 0.01 - ETA: 44:51 - loss: 1.0672 - dice_coef: 0.01 - ETA: 44:43 - loss: 1.0670 - dice_coef: 0.01 - ETA: 44:36 - loss: 1.0665 - dice_coef: 0.01 - ETA: 44:28 - loss: 1.0663 - dice_coef: 0.01 - ETA: 44:21 - loss: 1.0659 - dice_coef: 0.01 - ETA: 44:13 - loss: 1.0655 - dice_coef: 0.01 - ETA: 44:06 - loss: 1.0651 - dice_coef: 0.01 - ETA: 43:58 - loss: 1.0649 - dice_coef: 0.01 - ETA: 43:51 - loss: 1.0644 - dice_coef: 0.01 - ETA: 43:43 - loss: 1.0642 - dice_coef: 0.01 - ETA: 43:36 - loss: 1.0641 - dice_coef: 0.0182"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523/708 [=====================>........] - ETA: 43:28 - loss: 1.0643 - dice_coef: 0.01 - ETA: 43:21 - loss: 1.0641 - dice_coef: 0.01 - ETA: 43:14 - loss: 1.0637 - dice_coef: 0.01 - ETA: 43:06 - loss: 1.0633 - dice_coef: 0.01 - ETA: 42:59 - loss: 1.0630 - dice_coef: 0.01 - ETA: 42:51 - loss: 1.0626 - dice_coef: 0.01 - ETA: 42:44 - loss: 1.0622 - dice_coef: 0.02 - ETA: 42:36 - loss: 1.0620 - dice_coef: 0.02 - ETA: 42:29 - loss: 1.0618 - dice_coef: 0.02 - ETA: 42:22 - loss: 1.0607 - dice_coef: 0.02 - ETA: 42:14 - loss: 1.0602 - dice_coef: 0.02 - ETA: 42:07 - loss: 1.0593 - dice_coef: 0.02 - ETA: 42:00 - loss: 1.0592 - dice_coef: 0.02 - ETA: 41:52 - loss: 1.0588 - dice_coef: 0.02 - ETA: 41:45 - loss: 1.0595 - dice_coef: 0.02 - ETA: 41:37 - loss: 1.0594 - dice_coef: 0.02 - ETA: 41:30 - loss: 1.0590 - dice_coef: 0.02 - ETA: 41:23 - loss: 1.0588 - dice_coef: 0.02 - ETA: 41:15 - loss: 1.0586 - dice_coef: 0.02 - ETA: 41:08 - loss: 1.0586 - dice_coef: 0.02 - ETA: 41:00 - loss: 1.0585 - dice_coef: 0.02 - ETA: 40:53 - loss: 1.0584 - dice_coef: 0.02 - ETA: 40:45 - loss: 1.0584 - dice_coef: 0.02 - ETA: 40:38 - loss: 1.0580 - dice_coef: 0.02 - ETA: 40:30 - loss: 1.0577 - dice_coef: 0.02 - ETA: 40:23 - loss: 1.0576 - dice_coef: 0.02 - ETA: 40:16 - loss: 1.0574 - dice_coef: 0.02 - ETA: 40:08 - loss: 1.0574 - dice_coef: 0.02 - ETA: 40:01 - loss: 1.0572 - dice_coef: 0.02 - ETA: 39:53 - loss: 1.0571 - dice_coef: 0.02 - ETA: 39:46 - loss: 1.0568 - dice_coef: 0.02 - ETA: 39:39 - loss: 1.0564 - dice_coef: 0.02 - ETA: 39:31 - loss: 1.0561 - dice_coef: 0.02 - ETA: 39:24 - loss: 1.0558 - dice_coef: 0.02 - ETA: 39:16 - loss: 1.0554 - dice_coef: 0.02 - ETA: 39:09 - loss: 1.0552 - dice_coef: 0.02 - ETA: 39:02 - loss: 1.0548 - dice_coef: 0.02 - ETA: 38:54 - loss: 1.0547 - dice_coef: 0.02 - ETA: 38:47 - loss: 1.0545 - dice_coef: 0.02 - ETA: 38:39 - loss: 1.0538 - dice_coef: 0.02 - ETA: 38:32 - loss: 1.0532 - dice_coef: 0.02 - ETA: 38:25 - loss: 1.0530 - dice_coef: 0.02 - ETA: 38:17 - loss: 1.0529 - dice_coef: 0.02 - ETA: 38:10 - loss: 1.0526 - dice_coef: 0.02 - ETA: 38:03 - loss: 1.0527 - dice_coef: 0.02 - ETA: 37:55 - loss: 1.0525 - dice_coef: 0.02 - ETA: 37:48 - loss: 1.0523 - dice_coef: 0.02 - ETA: 37:40 - loss: 1.0520 - dice_coef: 0.03 - ETA: 37:33 - loss: 1.0514 - dice_coef: 0.03 - ETA: 37:26 - loss: 1.0513 - dice_coef: 0.03 - ETA: 37:19 - loss: 1.0510 - dice_coef: 0.03 - ETA: 37:11 - loss: 1.0509 - dice_coef: 0.03 - ETA: 37:04 - loss: 1.0508 - dice_coef: 0.03 - ETA: 36:56 - loss: 1.0505 - dice_coef: 0.03 - ETA: 36:49 - loss: 1.0499 - dice_coef: 0.03 - ETA: 36:42 - loss: 1.0493 - dice_coef: 0.03 - ETA: 36:34 - loss: 1.0492 - dice_coef: 0.03 - ETA: 36:27 - loss: 1.0490 - dice_coef: 0.03 - ETA: 36:20 - loss: 1.0489 - dice_coef: 0.03 - ETA: 36:12 - loss: 1.0487 - dice_coef: 0.03 - ETA: 36:05 - loss: 1.0486 - dice_coef: 0.03 - ETA: 35:58 - loss: 1.0483 - dice_coef: 0.03 - ETA: 35:50 - loss: 1.0482 - dice_coef: 0.03 - ETA: 35:43 - loss: 1.0481 - dice_coef: 0.03 - ETA: 35:36 - loss: 1.0478 - dice_coef: 0.03 - ETA: 35:29 - loss: 1.0478 - dice_coef: 0.03 - ETA: 35:22 - loss: 1.0476 - dice_coef: 0.03 - ETA: 35:14 - loss: 1.0475 - dice_coef: 0.03 - ETA: 35:07 - loss: 1.0471 - dice_coef: 0.03 - ETA: 35:00 - loss: 1.0467 - dice_coef: 0.03 - ETA: 34:52 - loss: 1.0467 - dice_coef: 0.03 - ETA: 34:45 - loss: 1.0464 - dice_coef: 0.03 - ETA: 34:38 - loss: 1.0461 - dice_coef: 0.03 - ETA: 34:30 - loss: 1.0460 - dice_coef: 0.03 - ETA: 34:23 - loss: 1.0456 - dice_coef: 0.03 - ETA: 34:16 - loss: 1.0456 - dice_coef: 0.03 - ETA: 34:08 - loss: 1.0456 - dice_coef: 0.03 - ETA: 34:01 - loss: 1.0457 - dice_coef: 0.03 - ETA: 33:54 - loss: 1.0454 - dice_coef: 0.03 - ETA: 33:47 - loss: 1.0451 - dice_coef: 0.03 - ETA: 33:40 - loss: 1.0450 - dice_coef: 0.03 - ETA: 33:32 - loss: 1.0448 - dice_coef: 0.03 - ETA: 33:25 - loss: 1.0448 - dice_coef: 0.03 - ETA: 33:18 - loss: 1.0446 - dice_coef: 0.03 - ETA: 33:11 - loss: 1.0444 - dice_coef: 0.03 - ETA: 33:03 - loss: 1.0442 - dice_coef: 0.03 - ETA: 32:56 - loss: 1.0441 - dice_coef: 0.03 - ETA: 32:49 - loss: 1.0441 - dice_coef: 0.03 - ETA: 32:42 - loss: 1.0437 - dice_coef: 0.03 - ETA: 32:35 - loss: 1.0433 - dice_coef: 0.03 - ETA: 32:27 - loss: 1.0430 - dice_coef: 0.03 - ETA: 32:20 - loss: 1.0430 - dice_coef: 0.03 - ETA: 32:13 - loss: 1.0427 - dice_coef: 0.03 - ETA: 32:06 - loss: 1.0426 - dice_coef: 0.03 - ETA: 31:59 - loss: 1.0424 - dice_coef: 0.03 - ETA: 31:52 - loss: 1.0423 - dice_coef: 0.03 - ETA: 31:44 - loss: 1.0421 - dice_coef: 0.03 - ETA: 31:37 - loss: 1.0419 - dice_coef: 0.03 - ETA: 31:30 - loss: 1.0418 - dice_coef: 0.03 - ETA: 31:23 - loss: 1.0416 - dice_coef: 0.03 - ETA: 31:16 - loss: 1.0416 - dice_coef: 0.03 - ETA: 31:08 - loss: 1.0415 - dice_coef: 0.03 - ETA: 31:01 - loss: 1.0407 - dice_coef: 0.04 - ETA: 30:54 - loss: 1.0408 - dice_coef: 0.04 - ETA: 30:47 - loss: 1.0406 - dice_coef: 0.04 - ETA: 30:39 - loss: 1.0406 - dice_coef: 0.04 - ETA: 30:32 - loss: 1.0405 - dice_coef: 0.04 - ETA: 30:25 - loss: 1.0402 - dice_coef: 0.04 - ETA: 30:17 - loss: 1.0402 - dice_coef: 0.04 - ETA: 30:10 - loss: 1.0403 - dice_coef: 0.04 - ETA: 30:03 - loss: 1.0402 - dice_coef: 0.04 - ETA: 29:56 - loss: 1.0396 - dice_coef: 0.04 - ETA: 29:48 - loss: 1.0396 - dice_coef: 0.04 - ETA: 29:41 - loss: 1.0393 - dice_coef: 0.04 - ETA: 29:34 - loss: 1.0395 - dice_coef: 0.04 - ETA: 29:27 - loss: 1.0394 - dice_coef: 0.04 - ETA: 29:19 - loss: 1.0396 - dice_coef: 0.04 - ETA: 29:12 - loss: 1.0396 - dice_coef: 0.04 - ETA: 29:05 - loss: 1.0393 - dice_coef: 0.04 - ETA: 28:58 - loss: 1.0388 - dice_coef: 0.04 - ETA: 28:50 - loss: 1.0382 - dice_coef: 0.04 - ETA: 28:43 - loss: 1.0379 - dice_coef: 0.04 - ETA: 28:36 - loss: 1.0377 - dice_coef: 0.04 - ETA: 28:29 - loss: 1.0375 - dice_coef: 0.04 - ETA: 28:22 - loss: 1.0375 - dice_coef: 0.04 - ETA: 28:14 - loss: 1.0375 - dice_coef: 0.04 - ETA: 28:07 - loss: 1.0373 - dice_coef: 0.04 - ETA: 28:00 - loss: 1.0373 - dice_coef: 0.04 - ETA: 27:53 - loss: 1.0371 - dice_coef: 0.04 - ETA: 27:46 - loss: 1.0369 - dice_coef: 0.04 - ETA: 27:39 - loss: 1.0368 - dice_coef: 0.04 - ETA: 27:32 - loss: 1.0367 - dice_coef: 0.04 - ETA: 27:25 - loss: 1.0364 - dice_coef: 0.04 - ETA: 27:18 - loss: 1.0364 - dice_coef: 0.04 - ETA: 27:11 - loss: 1.0363 - dice_coef: 0.04 - ETA: 27:04 - loss: 1.0362 - dice_coef: 0.04 - ETA: 26:56 - loss: 1.0362 - dice_coef: 0.04 - ETA: 26:49 - loss: 1.0360 - dice_coef: 0.04 - ETA: 26:42 - loss: 1.0360 - dice_coef: 0.04 - ETA: 26:35 - loss: 1.0359 - dice_coef: 0.04 - ETA: 26:28 - loss: 1.0357 - dice_coef: 0.04 - ETA: 26:21 - loss: 1.0356 - dice_coef: 0.04 - ETA: 26:13 - loss: 1.0354 - dice_coef: 0.04 - ETA: 26:06 - loss: 1.0351 - dice_coef: 0.04 - ETA: 25:59 - loss: 1.0348 - dice_coef: 0.04 - ETA: 25:52 - loss: 1.0345 - dice_coef: 0.04 - ETA: 25:45 - loss: 1.0338 - dice_coef: 0.04 - ETA: 25:37 - loss: 1.0337 - dice_coef: 0.04 - ETA: 25:30 - loss: 1.0337 - dice_coef: 0.04 - ETA: 25:23 - loss: 1.0336 - dice_coef: 0.04 - ETA: 25:16 - loss: 1.0333 - dice_coef: 0.04 - ETA: 25:08 - loss: 1.0330 - dice_coef: 0.04 - ETA: 25:01 - loss: 1.0329 - dice_coef: 0.04 - ETA: 24:54 - loss: 1.0330 - dice_coef: 0.04 - ETA: 24:47 - loss: 1.0328 - dice_coef: 0.04 - ETA: 24:39 - loss: 1.0327 - dice_coef: 0.04 - ETA: 24:32 - loss: 1.0324 - dice_coef: 0.04 - ETA: 24:25 - loss: 1.0322 - dice_coef: 0.04 - ETA: 24:18 - loss: 1.0320 - dice_coef: 0.04 - ETA: 24:11 - loss: 1.0314 - dice_coef: 0.04 - ETA: 24:03 - loss: 1.0311 - dice_coef: 0.05 - ETA: 23:56 - loss: 1.0307 - dice_coef: 0.05 - ETA: 23:49 - loss: 1.0307 - dice_coef: 0.05 - ETA: 23:42 - loss: 1.0307 - dice_coef: 0.05 - ETA: 23:34 - loss: 1.0306 - dice_coef: 0.05 - ETA: 23:27 - loss: 1.0305 - dice_coef: 0.05 - ETA: 23:20 - loss: 1.0299 - dice_coef: 0.05 - ETA: 23:13 - loss: 1.0297 - dice_coef: 0.05 - ETA: 23:05 - loss: 1.0290 - dice_coef: 0.05 - ETA: 22:58 - loss: 1.0288 - dice_coef: 0.05 - ETA: 22:51 - loss: 1.0285 - dice_coef: 0.05 - ETA: 22:44 - loss: 1.0283 - dice_coef: 0.05 - ETA: 22:37 - loss: 1.0285 - dice_coef: 0.05 - ETA: 22:29 - loss: 1.0285 - dice_coef: 0.05 - ETA: 22:22 - loss: 1.0284 - dice_coef: 0.05 - ETA: 22:15 - loss: 1.0284 - dice_coef: 0.05 - ETA: 22:08 - loss: 1.0281 - dice_coef: 0.05 - ETA: 22:01 - loss: 1.0280 - dice_coef: 0.0532"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/708 [============================>.] - ETA: 21:53 - loss: 1.0278 - dice_coef: 0.05 - ETA: 21:46 - loss: 1.0277 - dice_coef: 0.05 - ETA: 21:39 - loss: 1.0275 - dice_coef: 0.05 - ETA: 21:32 - loss: 1.0275 - dice_coef: 0.05 - ETA: 21:25 - loss: 1.0271 - dice_coef: 0.05 - ETA: 21:18 - loss: 1.0270 - dice_coef: 0.05 - ETA: 21:10 - loss: 1.0268 - dice_coef: 0.05 - ETA: 21:03 - loss: 1.0269 - dice_coef: 0.05 - ETA: 20:56 - loss: 1.0266 - dice_coef: 0.05 - ETA: 20:49 - loss: 1.0266 - dice_coef: 0.05 - ETA: 20:42 - loss: 1.0266 - dice_coef: 0.05 - ETA: 20:34 - loss: 1.0263 - dice_coef: 0.05 - ETA: 20:27 - loss: 1.0263 - dice_coef: 0.05 - ETA: 20:20 - loss: 1.0263 - dice_coef: 0.05 - ETA: 20:13 - loss: 1.0263 - dice_coef: 0.05 - ETA: 20:06 - loss: 1.0263 - dice_coef: 0.05 - ETA: 19:58 - loss: 1.0263 - dice_coef: 0.05 - ETA: 19:51 - loss: 1.0260 - dice_coef: 0.05 - ETA: 19:44 - loss: 1.0259 - dice_coef: 0.05 - ETA: 19:37 - loss: 1.0258 - dice_coef: 0.05 - ETA: 19:30 - loss: 1.0257 - dice_coef: 0.05 - ETA: 19:23 - loss: 1.0253 - dice_coef: 0.05 - ETA: 19:15 - loss: 1.0252 - dice_coef: 0.05 - ETA: 19:08 - loss: 1.0251 - dice_coef: 0.05 - ETA: 19:01 - loss: 1.0250 - dice_coef: 0.05 - ETA: 18:54 - loss: 1.0251 - dice_coef: 0.05 - ETA: 18:47 - loss: 1.0245 - dice_coef: 0.05 - ETA: 18:39 - loss: 1.0246 - dice_coef: 0.05 - ETA: 18:32 - loss: 1.0245 - dice_coef: 0.05 - ETA: 18:25 - loss: 1.0242 - dice_coef: 0.05 - ETA: 18:18 - loss: 1.0242 - dice_coef: 0.05 - ETA: 18:11 - loss: 1.0239 - dice_coef: 0.05 - ETA: 18:04 - loss: 1.0238 - dice_coef: 0.05 - ETA: 17:56 - loss: 1.0235 - dice_coef: 0.05 - ETA: 17:49 - loss: 1.0234 - dice_coef: 0.05 - ETA: 17:42 - loss: 1.0233 - dice_coef: 0.05 - ETA: 17:35 - loss: 1.0231 - dice_coef: 0.05 - ETA: 17:28 - loss: 1.0229 - dice_coef: 0.05 - ETA: 17:21 - loss: 1.0226 - dice_coef: 0.05 - ETA: 17:13 - loss: 1.0225 - dice_coef: 0.05 - ETA: 17:06 - loss: 1.0223 - dice_coef: 0.05 - ETA: 16:59 - loss: 1.0221 - dice_coef: 0.05 - ETA: 16:52 - loss: 1.0216 - dice_coef: 0.05 - ETA: 16:45 - loss: 1.0215 - dice_coef: 0.05 - ETA: 16:38 - loss: 1.0211 - dice_coef: 0.05 - ETA: 16:31 - loss: 1.0211 - dice_coef: 0.06 - ETA: 16:24 - loss: 1.0209 - dice_coef: 0.06 - ETA: 16:16 - loss: 1.0206 - dice_coef: 0.06 - ETA: 16:09 - loss: 1.0206 - dice_coef: 0.06 - ETA: 16:02 - loss: 1.0206 - dice_coef: 0.06 - ETA: 15:55 - loss: 1.0205 - dice_coef: 0.06 - ETA: 15:48 - loss: 1.0205 - dice_coef: 0.06 - ETA: 15:41 - loss: 1.0207 - dice_coef: 0.06 - ETA: 15:34 - loss: 1.0207 - dice_coef: 0.06 - ETA: 15:26 - loss: 1.0210 - dice_coef: 0.05 - ETA: 15:19 - loss: 1.0210 - dice_coef: 0.05 - ETA: 15:12 - loss: 1.0210 - dice_coef: 0.05 - ETA: 15:05 - loss: 1.0211 - dice_coef: 0.05 - ETA: 14:58 - loss: 1.0206 - dice_coef: 0.06 - ETA: 14:51 - loss: 1.0206 - dice_coef: 0.06 - ETA: 14:44 - loss: 1.0206 - dice_coef: 0.05 - ETA: 14:36 - loss: 1.0206 - dice_coef: 0.05 - ETA: 14:29 - loss: 1.0205 - dice_coef: 0.05 - ETA: 14:22 - loss: 1.0205 - dice_coef: 0.05 - ETA: 14:15 - loss: 1.0204 - dice_coef: 0.05 - ETA: 14:08 - loss: 1.0203 - dice_coef: 0.06 - ETA: 14:01 - loss: 1.0203 - dice_coef: 0.06 - ETA: 13:54 - loss: 1.0203 - dice_coef: 0.06 - ETA: 13:47 - loss: 1.0202 - dice_coef: 0.06 - ETA: 13:40 - loss: 1.0201 - dice_coef: 0.06 - ETA: 13:32 - loss: 1.0202 - dice_coef: 0.06 - ETA: 13:25 - loss: 1.0202 - dice_coef: 0.06 - ETA: 13:18 - loss: 1.0202 - dice_coef: 0.06 - ETA: 13:11 - loss: 1.0201 - dice_coef: 0.06 - ETA: 13:04 - loss: 1.0200 - dice_coef: 0.06 - ETA: 12:57 - loss: 1.0199 - dice_coef: 0.06 - ETA: 12:50 - loss: 1.0197 - dice_coef: 0.06 - ETA: 12:43 - loss: 1.0196 - dice_coef: 0.06 - ETA: 12:35 - loss: 1.0195 - dice_coef: 0.06 - ETA: 12:28 - loss: 1.0193 - dice_coef: 0.06 - ETA: 12:21 - loss: 1.0194 - dice_coef: 0.06 - ETA: 12:14 - loss: 1.0192 - dice_coef: 0.06 - ETA: 12:07 - loss: 1.0192 - dice_coef: 0.06 - ETA: 12:00 - loss: 1.0191 - dice_coef: 0.06 - ETA: 11:53 - loss: 1.0189 - dice_coef: 0.06 - ETA: 11:46 - loss: 1.0188 - dice_coef: 0.06 - ETA: 11:39 - loss: 1.0184 - dice_coef: 0.06 - ETA: 11:31 - loss: 1.0181 - dice_coef: 0.06 - ETA: 11:24 - loss: 1.0182 - dice_coef: 0.06 - ETA: 11:17 - loss: 1.0181 - dice_coef: 0.06 - ETA: 11:10 - loss: 1.0181 - dice_coef: 0.06 - ETA: 11:03 - loss: 1.0178 - dice_coef: 0.06 - ETA: 10:56 - loss: 1.0175 - dice_coef: 0.06 - ETA: 10:48 - loss: 1.0170 - dice_coef: 0.06 - ETA: 10:41 - loss: 1.0166 - dice_coef: 0.06 - ETA: 10:34 - loss: 1.0163 - dice_coef: 0.06 - ETA: 10:27 - loss: 1.0163 - dice_coef: 0.06 - ETA: 10:20 - loss: 1.0163 - dice_coef: 0.06 - ETA: 10:13 - loss: 1.0160 - dice_coef: 0.06 - ETA: 10:06 - loss: 1.0160 - dice_coef: 0.06 - ETA: 9:58 - loss: 1.0161 - dice_coef: 0.0641 - ETA: 9:51 - loss: 1.0159 - dice_coef: 0.064 - ETA: 9:44 - loss: 1.0159 - dice_coef: 0.064 - ETA: 9:37 - loss: 1.0159 - dice_coef: 0.064 - ETA: 9:30 - loss: 1.0155 - dice_coef: 0.064 - ETA: 9:23 - loss: 1.0150 - dice_coef: 0.064 - ETA: 9:15 - loss: 1.0151 - dice_coef: 0.064 - ETA: 9:08 - loss: 1.0150 - dice_coef: 0.064 - ETA: 9:01 - loss: 1.0149 - dice_coef: 0.064 - ETA: 8:54 - loss: 1.0149 - dice_coef: 0.064 - ETA: 8:47 - loss: 1.0146 - dice_coef: 0.065 - ETA: 8:40 - loss: 1.0142 - dice_coef: 0.065 - ETA: 8:33 - loss: 1.0142 - dice_coef: 0.065 - ETA: 8:25 - loss: 1.0139 - dice_coef: 0.065 - ETA: 8:18 - loss: 1.0138 - dice_coef: 0.066 - ETA: 8:11 - loss: 1.0137 - dice_coef: 0.066 - ETA: 8:04 - loss: 1.0135 - dice_coef: 0.066 - ETA: 7:57 - loss: 1.0133 - dice_coef: 0.066 - ETA: 7:50 - loss: 1.0132 - dice_coef: 0.066 - ETA: 7:43 - loss: 1.0131 - dice_coef: 0.066 - ETA: 7:35 - loss: 1.0131 - dice_coef: 0.066 - ETA: 7:28 - loss: 1.0131 - dice_coef: 0.066 - ETA: 7:21 - loss: 1.0132 - dice_coef: 0.066 - ETA: 7:14 - loss: 1.0132 - dice_coef: 0.066 - ETA: 7:07 - loss: 1.0131 - dice_coef: 0.066 - ETA: 7:00 - loss: 1.0131 - dice_coef: 0.066 - ETA: 6:53 - loss: 1.0130 - dice_coef: 0.066 - ETA: 6:46 - loss: 1.0130 - dice_coef: 0.066 - ETA: 6:38 - loss: 1.0130 - dice_coef: 0.066 - ETA: 6:31 - loss: 1.0128 - dice_coef: 0.066 - ETA: 6:24 - loss: 1.0128 - dice_coef: 0.066 - ETA: 6:17 - loss: 1.0126 - dice_coef: 0.066 - ETA: 6:10 - loss: 1.0125 - dice_coef: 0.066 - ETA: 6:03 - loss: 1.0124 - dice_coef: 0.066 - ETA: 5:56 - loss: 1.0123 - dice_coef: 0.066 - ETA: 5:49 - loss: 1.0126 - dice_coef: 0.066 - ETA: 5:42 - loss: 1.0124 - dice_coef: 0.067 - ETA: 5:34 - loss: 1.0124 - dice_coef: 0.067 - ETA: 5:27 - loss: 1.0124 - dice_coef: 0.067 - ETA: 5:20 - loss: 1.0122 - dice_coef: 0.067 - ETA: 5:13 - loss: 1.0121 - dice_coef: 0.067 - ETA: 5:06 - loss: 1.0121 - dice_coef: 0.067 - ETA: 4:59 - loss: 1.0116 - dice_coef: 0.067 - ETA: 4:52 - loss: 1.0115 - dice_coef: 0.068 - ETA: 4:44 - loss: 1.0114 - dice_coef: 0.068 - ETA: 4:37 - loss: 1.0113 - dice_coef: 0.068 - ETA: 4:30 - loss: 1.0112 - dice_coef: 0.068 - ETA: 4:23 - loss: 1.0110 - dice_coef: 0.068 - ETA: 4:16 - loss: 1.0106 - dice_coef: 0.068 - ETA: 4:09 - loss: 1.0103 - dice_coef: 0.069 - ETA: 4:02 - loss: 1.0099 - dice_coef: 0.069 - ETA: 3:55 - loss: 1.0094 - dice_coef: 0.070 - ETA: 3:48 - loss: 1.0090 - dice_coef: 0.070 - ETA: 3:40 - loss: 1.0090 - dice_coef: 0.070 - ETA: 3:33 - loss: 1.0085 - dice_coef: 0.071 - ETA: 3:26 - loss: 1.0081 - dice_coef: 0.071 - ETA: 3:19 - loss: 1.0080 - dice_coef: 0.071 - ETA: 3:12 - loss: 1.0080 - dice_coef: 0.071 - ETA: 3:05 - loss: 1.0077 - dice_coef: 0.071 - ETA: 2:58 - loss: 1.0075 - dice_coef: 0.072 - ETA: 2:51 - loss: 1.0074 - dice_coef: 0.072 - ETA: 2:43 - loss: 1.0072 - dice_coef: 0.072 - ETA: 2:36 - loss: 1.0070 - dice_coef: 0.072 - ETA: 2:29 - loss: 1.0069 - dice_coef: 0.072 - ETA: 2:22 - loss: 1.0069 - dice_coef: 0.072 - ETA: 2:15 - loss: 1.0069 - dice_coef: 0.072 - ETA: 2:08 - loss: 1.0063 - dice_coef: 0.073 - ETA: 2:01 - loss: 1.0065 - dice_coef: 0.073 - ETA: 1:54 - loss: 1.0065 - dice_coef: 0.073 - ETA: 1:46 - loss: 1.0065 - dice_coef: 0.073 - ETA: 1:39 - loss: 1.0061 - dice_coef: 0.073 - ETA: 1:32 - loss: 1.0060 - dice_coef: 0.073 - ETA: 1:25 - loss: 1.0060 - dice_coef: 0.073 - ETA: 1:18 - loss: 1.0058 - dice_coef: 0.073 - ETA: 1:11 - loss: 1.0051 - dice_coef: 0.074 - ETA: 1:04 - loss: 1.0049 - dice_coef: 0.074 - ETA: 57s - loss: 1.0047 - dice_coef: 0.074 - ETA: 49s - loss: 1.0046 - dice_coef: 0.0748"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708/708 [==============================] - ETA: 42s - loss: 1.0044 - dice_coef: 0.07 - ETA: 35s - loss: 1.0046 - dice_coef: 0.07 - ETA: 28s - loss: 1.0047 - dice_coef: 0.07 - ETA: 21s - loss: 1.0046 - dice_coef: 0.07 - ETA: 14s - loss: 1.0043 - dice_coef: 0.07 - ETA: 7s - loss: 1.0036 - dice_coef: 0.0761 - 5396s 8s/step - loss: 1.0033 - dice_coef: 0.0764 - val_loss: 0.8764 - val_dice_coef: 0.2028\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/708 [======>.......................] - ETA: 1:22:22 - loss: 0.7346 - dice_coef: 0.32 - ETA: 1:22:07 - loss: 0.8196 - dice_coef: 0.26 - ETA: 1:21:54 - loss: 0.8086 - dice_coef: 0.28 - ETA: 1:21:44 - loss: 0.8090 - dice_coef: 0.27 - ETA: 1:21:44 - loss: 0.7965 - dice_coef: 0.29 - ETA: 1:22:11 - loss: 0.8305 - dice_coef: 0.28 - ETA: 1:24:07 - loss: 0.8474 - dice_coef: 0.25 - ETA: 1:24:29 - loss: 0.8644 - dice_coef: 0.23 - ETA: 1:25:02 - loss: 0.8497 - dice_coef: 0.24 - ETA: 1:25:00 - loss: 0.8508 - dice_coef: 0.24 - ETA: 1:25:04 - loss: 0.8330 - dice_coef: 0.25 - ETA: 1:24:40 - loss: 0.8487 - dice_coef: 0.23 - ETA: 1:24:14 - loss: 0.8603 - dice_coef: 0.22 - ETA: 1:24:22 - loss: 0.8527 - dice_coef: 0.23 - ETA: 1:24:30 - loss: 0.8616 - dice_coef: 0.21 - ETA: 1:24:44 - loss: 0.8561 - dice_coef: 0.22 - ETA: 1:24:58 - loss: 0.8514 - dice_coef: 0.22 - ETA: 1:25:43 - loss: 0.8577 - dice_coef: 0.22 - ETA: 1:26:10 - loss: 0.8514 - dice_coef: 0.22 - ETA: 1:26:35 - loss: 0.8538 - dice_coef: 0.22 - ETA: 1:26:55 - loss: 0.8513 - dice_coef: 0.22 - ETA: 1:27:16 - loss: 0.8518 - dice_coef: 0.22 - ETA: 1:27:34 - loss: 0.8478 - dice_coef: 0.22 - ETA: 1:27:30 - loss: 0.8302 - dice_coef: 0.24 - ETA: 1:27:25 - loss: 0.8258 - dice_coef: 0.24 - ETA: 1:27:31 - loss: 0.8316 - dice_coef: 0.24 - ETA: 1:27:32 - loss: 0.8391 - dice_coef: 0.23 - ETA: 1:27:29 - loss: 0.8447 - dice_coef: 0.23 - ETA: 1:27:34 - loss: 0.8450 - dice_coef: 0.23 - ETA: 1:27:31 - loss: 0.8464 - dice_coef: 0.22 - ETA: 1:27:14 - loss: 0.8369 - dice_coef: 0.23 - ETA: 1:27:14 - loss: 0.8455 - dice_coef: 0.23 - ETA: 1:27:15 - loss: 0.8462 - dice_coef: 0.23 - ETA: 1:27:03 - loss: 0.8503 - dice_coef: 0.22 - ETA: 1:26:48 - loss: 0.8500 - dice_coef: 0.22 - ETA: 1:26:42 - loss: 0.8481 - dice_coef: 0.22 - ETA: 1:26:43 - loss: 0.8425 - dice_coef: 0.23 - ETA: 1:26:41 - loss: 0.8456 - dice_coef: 0.22 - ETA: 1:26:40 - loss: 0.8511 - dice_coef: 0.22 - ETA: 1:26:28 - loss: 0.8521 - dice_coef: 0.22 - ETA: 1:26:26 - loss: 0.8545 - dice_coef: 0.21 - ETA: 1:26:23 - loss: 0.8584 - dice_coef: 0.21 - ETA: 1:26:12 - loss: 0.8592 - dice_coef: 0.21 - ETA: 1:26:02 - loss: 0.8539 - dice_coef: 0.21 - ETA: 1:25:50 - loss: 0.8531 - dice_coef: 0.21 - ETA: 1:25:40 - loss: 0.8487 - dice_coef: 0.22 - ETA: 1:25:37 - loss: 0.8394 - dice_coef: 0.23 - ETA: 1:25:25 - loss: 0.8402 - dice_coef: 0.23 - ETA: 1:25:18 - loss: 0.8459 - dice_coef: 0.22 - ETA: 1:25:15 - loss: 0.8467 - dice_coef: 0.22 - ETA: 1:25:12 - loss: 0.8507 - dice_coef: 0.22 - ETA: 1:25:09 - loss: 0.8525 - dice_coef: 0.22 - ETA: 1:25:07 - loss: 0.8553 - dice_coef: 0.22 - ETA: 1:25:04 - loss: 0.8586 - dice_coef: 0.21 - ETA: 1:24:59 - loss: 0.8596 - dice_coef: 0.21 - ETA: 1:24:53 - loss: 0.8591 - dice_coef: 0.21 - ETA: 1:24:40 - loss: 0.8623 - dice_coef: 0.21 - ETA: 1:24:29 - loss: 0.8635 - dice_coef: 0.21 - ETA: 1:24:18 - loss: 0.8611 - dice_coef: 0.21 - ETA: 1:24:10 - loss: 0.8571 - dice_coef: 0.22 - ETA: 1:24:06 - loss: 0.8599 - dice_coef: 0.21 - ETA: 1:24:04 - loss: 0.8619 - dice_coef: 0.21 - ETA: 1:23:59 - loss: 0.8628 - dice_coef: 0.21 - ETA: 1:23:45 - loss: 0.8585 - dice_coef: 0.22 - ETA: 1:23:31 - loss: 0.8590 - dice_coef: 0.21 - ETA: 1:23:18 - loss: 0.8608 - dice_coef: 0.21 - ETA: 1:23:11 - loss: 0.8581 - dice_coef: 0.21 - ETA: 1:22:57 - loss: 0.8570 - dice_coef: 0.22 - ETA: 1:22:43 - loss: 0.8575 - dice_coef: 0.21 - ETA: 1:22:30 - loss: 0.8560 - dice_coef: 0.22 - ETA: 1:22:17 - loss: 0.8572 - dice_coef: 0.21 - ETA: 1:22:03 - loss: 0.8595 - dice_coef: 0.21 - ETA: 1:21:50 - loss: 0.8618 - dice_coef: 0.21 - ETA: 1:21:36 - loss: 0.8632 - dice_coef: 0.21 - ETA: 1:21:23 - loss: 0.8659 - dice_coef: 0.21 - ETA: 1:21:11 - loss: 0.8636 - dice_coef: 0.21 - ETA: 1:20:58 - loss: 0.8610 - dice_coef: 0.21 - ETA: 1:20:46 - loss: 0.8628 - dice_coef: 0.21 - ETA: 1:20:32 - loss: 0.8612 - dice_coef: 0.21 - ETA: 1:20:24 - loss: 0.8617 - dice_coef: 0.21 - ETA: 1:20:12 - loss: 0.8630 - dice_coef: 0.21 - ETA: 1:20:00 - loss: 0.8605 - dice_coef: 0.21 - ETA: 1:19:49 - loss: 0.8625 - dice_coef: 0.21 - ETA: 1:19:36 - loss: 0.8645 - dice_coef: 0.21 - ETA: 1:19:24 - loss: 0.8628 - dice_coef: 0.21 - ETA: 1:19:11 - loss: 0.8649 - dice_coef: 0.21 - ETA: 1:18:59 - loss: 0.8628 - dice_coef: 0.21 - ETA: 1:18:47 - loss: 0.8632 - dice_coef: 0.21 - ETA: 1:18:36 - loss: 0.8653 - dice_coef: 0.21 - ETA: 1:18:24 - loss: 0.8622 - dice_coef: 0.21 - ETA: 1:18:13 - loss: 0.8598 - dice_coef: 0.21 - ETA: 1:18:01 - loss: 0.8592 - dice_coef: 0.21 - ETA: 1:17:52 - loss: 0.8564 - dice_coef: 0.21 - ETA: 1:17:42 - loss: 0.8533 - dice_coef: 0.22 - ETA: 1:17:31 - loss: 0.8544 - dice_coef: 0.22 - ETA: 1:17:23 - loss: 0.8507 - dice_coef: 0.22 - ETA: 1:17:15 - loss: 0.8522 - dice_coef: 0.22 - ETA: 1:17:05 - loss: 0.8531 - dice_coef: 0.22 - ETA: 1:16:56 - loss: 0.8523 - dice_coef: 0.22 - ETA: 1:16:47 - loss: 0.8510 - dice_coef: 0.22 - ETA: 1:16:38 - loss: 0.8500 - dice_coef: 0.22 - ETA: 1:16:28 - loss: 0.8471 - dice_coef: 0.22 - ETA: 1:16:19 - loss: 0.8459 - dice_coef: 0.23 - ETA: 1:16:10 - loss: 0.8494 - dice_coef: 0.22 - ETA: 1:16:02 - loss: 0.8496 - dice_coef: 0.22 - ETA: 1:15:56 - loss: 0.8482 - dice_coef: 0.23 - ETA: 1:15:46 - loss: 0.8469 - dice_coef: 0.23 - ETA: 1:15:37 - loss: 0.8473 - dice_coef: 0.23 - ETA: 1:15:27 - loss: 0.8449 - dice_coef: 0.23 - ETA: 1:15:18 - loss: 0.8451 - dice_coef: 0.23 - ETA: 1:15:08 - loss: 0.8451 - dice_coef: 0.23 - ETA: 1:14:59 - loss: 0.8456 - dice_coef: 0.23 - ETA: 1:14:49 - loss: 0.8470 - dice_coef: 0.23 - ETA: 1:14:41 - loss: 0.8470 - dice_coef: 0.23 - ETA: 1:14:33 - loss: 0.8449 - dice_coef: 0.23 - ETA: 1:14:25 - loss: 0.8466 - dice_coef: 0.23 - ETA: 1:14:17 - loss: 0.8456 - dice_coef: 0.23 - ETA: 1:14:08 - loss: 0.8472 - dice_coef: 0.23 - ETA: 1:13:59 - loss: 0.8491 - dice_coef: 0.23 - ETA: 1:13:50 - loss: 0.8472 - dice_coef: 0.23 - ETA: 1:13:40 - loss: 0.8449 - dice_coef: 0.23 - ETA: 1:13:31 - loss: 0.8431 - dice_coef: 0.23 - ETA: 1:13:21 - loss: 0.8438 - dice_coef: 0.23 - ETA: 1:13:11 - loss: 0.8408 - dice_coef: 0.23 - ETA: 1:13:01 - loss: 0.8386 - dice_coef: 0.24 - ETA: 1:12:52 - loss: 0.8396 - dice_coef: 0.24 - ETA: 1:12:44 - loss: 0.8401 - dice_coef: 0.23 - ETA: 1:12:35 - loss: 0.8367 - dice_coef: 0.24 - ETA: 1:12:27 - loss: 0.8354 - dice_coef: 0.24 - ETA: 1:12:18 - loss: 0.8357 - dice_coef: 0.24 - ETA: 1:12:10 - loss: 0.8330 - dice_coef: 0.24 - ETA: 1:12:01 - loss: 0.8300 - dice_coef: 0.24 - ETA: 1:11:51 - loss: 0.8279 - dice_coef: 0.25 - ETA: 1:11:42 - loss: 0.8283 - dice_coef: 0.25 - ETA: 1:11:34 - loss: 0.8279 - dice_coef: 0.25 - ETA: 1:11:25 - loss: 0.8273 - dice_coef: 0.25 - ETA: 1:11:16 - loss: 0.8272 - dice_coef: 0.25 - ETA: 1:11:07 - loss: 0.8256 - dice_coef: 0.25 - ETA: 1:11:03 - loss: 0.8250 - dice_coef: 0.25 - ETA: 1:11:01 - loss: 0.8252 - dice_coef: 0.25 - ETA: 1:11:04 - loss: 0.8237 - dice_coef: 0.25 - ETA: 1:11:11 - loss: 0.8252 - dice_coef: 0.25 - ETA: 1:11:18 - loss: 0.8265 - dice_coef: 0.25 - ETA: 1:11:27 - loss: 0.8272 - dice_coef: 0.24 - ETA: 1:11:35 - loss: 0.8255 - dice_coef: 0.25 - ETA: 1:11:39 - loss: 0.8263 - dice_coef: 0.25 - ETA: 1:11:37 - loss: 0.8250 - dice_coef: 0.25 - ETA: 1:11:36 - loss: 0.8245 - dice_coef: 0.25 - ETA: 1:11:38 - loss: 0.8259 - dice_coef: 0.25 - ETA: 1:11:33 - loss: 0.8250 - dice_coef: 0.25 - ETA: 1:11:39 - loss: 0.8263 - dice_coef: 0.24 - ETA: 1:11:36 - loss: 0.8275 - dice_coef: 0.24 - ETA: 1:11:39 - loss: 0.8294 - dice_coef: 0.24 - ETA: 1:11:43 - loss: 0.8299 - dice_coef: 0.24 - ETA: 1:11:48 - loss: 0.8293 - dice_coef: 0.24 - ETA: 1:11:50 - loss: 0.8285 - dice_coef: 0.24 - ETA: 1:11:50 - loss: 0.8282 - dice_coef: 0.24 - ETA: 1:11:52 - loss: 0.8282 - dice_coef: 0.24 - ETA: 1:11:43 - loss: 0.8293 - dice_coef: 0.24 - ETA: 1:11:33 - loss: 0.8298 - dice_coef: 0.24 - ETA: 1:11:23 - loss: 0.8309 - dice_coef: 0.24 - ETA: 1:11:13 - loss: 0.8320 - dice_coef: 0.24 - ETA: 1:11:02 - loss: 0.8329 - dice_coef: 0.24 - ETA: 1:10:52 - loss: 0.8319 - dice_coef: 0.24 - ETA: 1:10:43 - loss: 0.8304 - dice_coef: 0.24 - ETA: 1:10:33 - loss: 0.8299 - dice_coef: 0.24 - ETA: 1:10:23 - loss: 0.8295 - dice_coef: 0.24 - ETA: 1:10:14 - loss: 0.8277 - dice_coef: 0.24 - ETA: 1:10:04 - loss: 0.8280 - dice_coef: 0.24 - ETA: 1:09:58 - loss: 0.8280 - dice_coef: 0.2476"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/708 [=============>................] - ETA: 1:09:49 - loss: 0.8279 - dice_coef: 0.24 - ETA: 1:09:41 - loss: 0.8272 - dice_coef: 0.24 - ETA: 1:09:33 - loss: 0.8271 - dice_coef: 0.24 - ETA: 1:09:24 - loss: 0.8264 - dice_coef: 0.24 - ETA: 1:09:15 - loss: 0.8274 - dice_coef: 0.24 - ETA: 1:09:05 - loss: 0.8273 - dice_coef: 0.24 - ETA: 1:08:55 - loss: 0.8273 - dice_coef: 0.24 - ETA: 1:08:45 - loss: 0.8265 - dice_coef: 0.24 - ETA: 1:08:36 - loss: 0.8242 - dice_coef: 0.25 - ETA: 1:08:26 - loss: 0.8237 - dice_coef: 0.25 - ETA: 1:08:16 - loss: 0.8243 - dice_coef: 0.24 - ETA: 1:08:06 - loss: 0.8243 - dice_coef: 0.25 - ETA: 1:07:56 - loss: 0.8238 - dice_coef: 0.25 - ETA: 1:07:46 - loss: 0.8249 - dice_coef: 0.25 - ETA: 1:07:36 - loss: 0.8245 - dice_coef: 0.25 - ETA: 1:07:26 - loss: 0.8255 - dice_coef: 0.24 - ETA: 1:07:16 - loss: 0.8264 - dice_coef: 0.24 - ETA: 1:07:07 - loss: 0.8273 - dice_coef: 0.24 - ETA: 1:06:57 - loss: 0.8263 - dice_coef: 0.24 - ETA: 1:06:47 - loss: 0.8262 - dice_coef: 0.24 - ETA: 1:06:37 - loss: 0.8271 - dice_coef: 0.24 - ETA: 1:06:28 - loss: 0.8282 - dice_coef: 0.24 - ETA: 1:06:18 - loss: 0.8287 - dice_coef: 0.24 - ETA: 1:06:09 - loss: 0.8288 - dice_coef: 0.24 - ETA: 1:06:00 - loss: 0.8280 - dice_coef: 0.24 - ETA: 1:05:51 - loss: 0.8283 - dice_coef: 0.24 - ETA: 1:05:42 - loss: 0.8283 - dice_coef: 0.24 - ETA: 1:05:34 - loss: 0.8281 - dice_coef: 0.24 - ETA: 1:05:25 - loss: 0.8287 - dice_coef: 0.24 - ETA: 1:05:16 - loss: 0.8278 - dice_coef: 0.24 - ETA: 1:05:07 - loss: 0.8258 - dice_coef: 0.24 - ETA: 1:04:58 - loss: 0.8265 - dice_coef: 0.24 - ETA: 1:04:49 - loss: 0.8271 - dice_coef: 0.24 - ETA: 1:04:40 - loss: 0.8280 - dice_coef: 0.24 - ETA: 1:04:31 - loss: 0.8271 - dice_coef: 0.24 - ETA: 1:04:22 - loss: 0.8251 - dice_coef: 0.25 - ETA: 1:04:12 - loss: 0.8244 - dice_coef: 0.25 - ETA: 1:04:03 - loss: 0.8231 - dice_coef: 0.25 - ETA: 1:03:54 - loss: 0.8234 - dice_coef: 0.25 - ETA: 1:03:45 - loss: 0.8229 - dice_coef: 0.25 - ETA: 1:03:36 - loss: 0.8224 - dice_coef: 0.25 - ETA: 1:03:28 - loss: 0.8211 - dice_coef: 0.25 - ETA: 1:03:19 - loss: 0.8212 - dice_coef: 0.25 - ETA: 1:03:10 - loss: 0.8204 - dice_coef: 0.25 - ETA: 1:03:02 - loss: 0.8198 - dice_coef: 0.25 - ETA: 1:02:53 - loss: 0.8201 - dice_coef: 0.25 - ETA: 1:02:43 - loss: 0.8187 - dice_coef: 0.25 - ETA: 1:02:35 - loss: 0.8196 - dice_coef: 0.25 - ETA: 1:02:26 - loss: 0.8195 - dice_coef: 0.25 - ETA: 1:02:17 - loss: 0.8191 - dice_coef: 0.25 - ETA: 1:02:08 - loss: 0.8184 - dice_coef: 0.25 - ETA: 1:01:59 - loss: 0.8170 - dice_coef: 0.25 - ETA: 1:01:50 - loss: 0.8173 - dice_coef: 0.25 - ETA: 1:01:41 - loss: 0.8177 - dice_coef: 0.25 - ETA: 1:01:32 - loss: 0.8181 - dice_coef: 0.25 - ETA: 1:01:23 - loss: 0.8171 - dice_coef: 0.25 - ETA: 1:01:15 - loss: 0.8172 - dice_coef: 0.25 - ETA: 1:01:06 - loss: 0.8182 - dice_coef: 0.25 - ETA: 1:00:57 - loss: 0.8186 - dice_coef: 0.25 - ETA: 1:00:49 - loss: 0.8194 - dice_coef: 0.25 - ETA: 1:00:40 - loss: 0.8201 - dice_coef: 0.25 - ETA: 1:00:32 - loss: 0.8207 - dice_coef: 0.25 - ETA: 1:00:24 - loss: 0.8204 - dice_coef: 0.25 - ETA: 1:00:17 - loss: 0.8210 - dice_coef: 0.25 - ETA: 1:00:09 - loss: 0.8204 - dice_coef: 0.25 - ETA: 1:00:02 - loss: 0.8205 - dice_coef: 0.25 - ETA: 59:53 - loss: 0.8193 - dice_coef: 0.2556 - ETA: 59:45 - loss: 0.8186 - dice_coef: 0.25 - ETA: 59:36 - loss: 0.8172 - dice_coef: 0.25 - ETA: 59:27 - loss: 0.8171 - dice_coef: 0.25 - ETA: 59:18 - loss: 0.8178 - dice_coef: 0.25 - ETA: 59:10 - loss: 0.8171 - dice_coef: 0.25 - ETA: 59:01 - loss: 0.8171 - dice_coef: 0.25 - ETA: 58:53 - loss: 0.8172 - dice_coef: 0.25 - ETA: 58:44 - loss: 0.8179 - dice_coef: 0.25 - ETA: 58:36 - loss: 0.8165 - dice_coef: 0.25 - ETA: 58:28 - loss: 0.8166 - dice_coef: 0.25 - ETA: 58:20 - loss: 0.8175 - dice_coef: 0.25 - ETA: 58:11 - loss: 0.8182 - dice_coef: 0.25 - ETA: 58:03 - loss: 0.8173 - dice_coef: 0.25 - ETA: 57:54 - loss: 0.8171 - dice_coef: 0.25 - ETA: 57:45 - loss: 0.8171 - dice_coef: 0.25 - ETA: 57:37 - loss: 0.8167 - dice_coef: 0.25 - ETA: 57:28 - loss: 0.8172 - dice_coef: 0.25 - ETA: 57:20 - loss: 0.8173 - dice_coef: 0.25 - ETA: 57:11 - loss: 0.8175 - dice_coef: 0.25 - ETA: 57:03 - loss: 0.8185 - dice_coef: 0.25 - ETA: 56:54 - loss: 0.8192 - dice_coef: 0.25 - ETA: 56:45 - loss: 0.8186 - dice_coef: 0.25 - ETA: 56:37 - loss: 0.8194 - dice_coef: 0.25 - ETA: 56:28 - loss: 0.8203 - dice_coef: 0.25 - ETA: 56:20 - loss: 0.8204 - dice_coef: 0.25 - ETA: 56:11 - loss: 0.8210 - dice_coef: 0.25 - ETA: 56:03 - loss: 0.8209 - dice_coef: 0.25 - ETA: 55:54 - loss: 0.8206 - dice_coef: 0.25 - ETA: 55:46 - loss: 0.8208 - dice_coef: 0.25 - ETA: 55:37 - loss: 0.8217 - dice_coef: 0.25 - ETA: 55:29 - loss: 0.8217 - dice_coef: 0.25 - ETA: 55:21 - loss: 0.8210 - dice_coef: 0.25 - ETA: 55:12 - loss: 0.8211 - dice_coef: 0.25 - ETA: 55:03 - loss: 0.8216 - dice_coef: 0.25 - ETA: 54:55 - loss: 0.8221 - dice_coef: 0.25 - ETA: 54:46 - loss: 0.8223 - dice_coef: 0.25 - ETA: 54:38 - loss: 0.8215 - dice_coef: 0.25 - ETA: 54:30 - loss: 0.8215 - dice_coef: 0.25 - ETA: 54:21 - loss: 0.8208 - dice_coef: 0.25 - ETA: 54:13 - loss: 0.8211 - dice_coef: 0.25 - ETA: 54:04 - loss: 0.8213 - dice_coef: 0.25 - ETA: 53:56 - loss: 0.8213 - dice_coef: 0.25 - ETA: 53:48 - loss: 0.8206 - dice_coef: 0.25 - ETA: 53:39 - loss: 0.8209 - dice_coef: 0.25 - ETA: 53:31 - loss: 0.8205 - dice_coef: 0.25 - ETA: 53:23 - loss: 0.8211 - dice_coef: 0.25 - ETA: 53:15 - loss: 0.8219 - dice_coef: 0.25 - ETA: 53:06 - loss: 0.8225 - dice_coef: 0.25 - ETA: 52:58 - loss: 0.8235 - dice_coef: 0.25 - ETA: 52:50 - loss: 0.8239 - dice_coef: 0.25 - ETA: 52:42 - loss: 0.8247 - dice_coef: 0.25 - ETA: 52:33 - loss: 0.8253 - dice_coef: 0.25 - ETA: 52:25 - loss: 0.8259 - dice_coef: 0.25 - ETA: 52:17 - loss: 0.8266 - dice_coef: 0.24 - ETA: 52:08 - loss: 0.8272 - dice_coef: 0.24 - ETA: 52:00 - loss: 0.8279 - dice_coef: 0.24 - ETA: 51:52 - loss: 0.8285 - dice_coef: 0.24 - ETA: 51:44 - loss: 0.8292 - dice_coef: 0.24 - ETA: 51:36 - loss: 0.8301 - dice_coef: 0.24 - ETA: 51:28 - loss: 0.8308 - dice_coef: 0.24 - ETA: 51:20 - loss: 0.8315 - dice_coef: 0.24 - ETA: 51:12 - loss: 0.8321 - dice_coef: 0.24 - ETA: 51:04 - loss: 0.8326 - dice_coef: 0.24 - ETA: 50:56 - loss: 0.8330 - dice_coef: 0.24 - ETA: 50:48 - loss: 0.8333 - dice_coef: 0.24 - ETA: 50:40 - loss: 0.8335 - dice_coef: 0.24 - ETA: 50:32 - loss: 0.8332 - dice_coef: 0.24 - ETA: 50:24 - loss: 0.8336 - dice_coef: 0.24 - ETA: 50:18 - loss: 0.8340 - dice_coef: 0.24 - ETA: 50:10 - loss: 0.8341 - dice_coef: 0.24 - ETA: 50:02 - loss: 0.8340 - dice_coef: 0.24 - ETA: 49:54 - loss: 0.8343 - dice_coef: 0.24 - ETA: 49:46 - loss: 0.8341 - dice_coef: 0.24 - ETA: 49:38 - loss: 0.8345 - dice_coef: 0.24 - ETA: 49:31 - loss: 0.8336 - dice_coef: 0.24 - ETA: 49:25 - loss: 0.8321 - dice_coef: 0.24 - ETA: 49:18 - loss: 0.8320 - dice_coef: 0.24 - ETA: 49:11 - loss: 0.8319 - dice_coef: 0.24 - ETA: 49:05 - loss: 0.8324 - dice_coef: 0.24 - ETA: 48:58 - loss: 0.8328 - dice_coef: 0.24 - ETA: 48:52 - loss: 0.8333 - dice_coef: 0.24 - ETA: 48:45 - loss: 0.8326 - dice_coef: 0.24 - ETA: 48:38 - loss: 0.8325 - dice_coef: 0.24 - ETA: 48:30 - loss: 0.8322 - dice_coef: 0.24 - ETA: 48:23 - loss: 0.8319 - dice_coef: 0.24 - ETA: 48:15 - loss: 0.8314 - dice_coef: 0.24 - ETA: 48:09 - loss: 0.8318 - dice_coef: 0.24 - ETA: 48:02 - loss: 0.8315 - dice_coef: 0.24 - ETA: 47:56 - loss: 0.8306 - dice_coef: 0.24 - ETA: 47:50 - loss: 0.8314 - dice_coef: 0.24 - ETA: 47:43 - loss: 0.8308 - dice_coef: 0.24 - ETA: 47:36 - loss: 0.8308 - dice_coef: 0.24 - ETA: 47:30 - loss: 0.8306 - dice_coef: 0.24 - ETA: 47:23 - loss: 0.8307 - dice_coef: 0.24 - ETA: 47:15 - loss: 0.8311 - dice_coef: 0.24 - ETA: 47:07 - loss: 0.8312 - dice_coef: 0.24 - ETA: 46:58 - loss: 0.8307 - dice_coef: 0.24 - ETA: 46:50 - loss: 0.8313 - dice_coef: 0.24 - ETA: 46:42 - loss: 0.8303 - dice_coef: 0.24 - ETA: 46:34 - loss: 0.8304 - dice_coef: 0.24 - ETA: 46:27 - loss: 0.8299 - dice_coef: 0.24 - ETA: 46:20 - loss: 0.8294 - dice_coef: 0.24 - ETA: 46:14 - loss: 0.8295 - dice_coef: 0.24 - ETA: 46:07 - loss: 0.8295 - dice_coef: 0.24 - ETA: 46:00 - loss: 0.8290 - dice_coef: 0.24 - ETA: 45:54 - loss: 0.8297 - dice_coef: 0.24 - ETA: 45:47 - loss: 0.8302 - dice_coef: 0.24 - ETA: 45:40 - loss: 0.8299 - dice_coef: 0.2471"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523/708 [=====================>........] - ETA: 45:33 - loss: 0.8296 - dice_coef: 0.24 - ETA: 45:27 - loss: 0.8294 - dice_coef: 0.24 - ETA: 45:20 - loss: 0.8289 - dice_coef: 0.24 - ETA: 45:13 - loss: 0.8289 - dice_coef: 0.24 - ETA: 45:06 - loss: 0.8285 - dice_coef: 0.24 - ETA: 44:59 - loss: 0.8273 - dice_coef: 0.24 - ETA: 44:52 - loss: 0.8273 - dice_coef: 0.24 - ETA: 44:45 - loss: 0.8260 - dice_coef: 0.25 - ETA: 44:39 - loss: 0.8259 - dice_coef: 0.25 - ETA: 44:32 - loss: 0.8258 - dice_coef: 0.25 - ETA: 44:25 - loss: 0.8257 - dice_coef: 0.25 - ETA: 44:18 - loss: 0.8262 - dice_coef: 0.25 - ETA: 44:12 - loss: 0.8261 - dice_coef: 0.25 - ETA: 44:05 - loss: 0.8257 - dice_coef: 0.25 - ETA: 43:58 - loss: 0.8262 - dice_coef: 0.24 - ETA: 43:51 - loss: 0.8265 - dice_coef: 0.24 - ETA: 43:44 - loss: 0.8265 - dice_coef: 0.24 - ETA: 43:37 - loss: 0.8261 - dice_coef: 0.24 - ETA: 43:30 - loss: 0.8261 - dice_coef: 0.24 - ETA: 43:23 - loss: 0.8261 - dice_coef: 0.24 - ETA: 43:16 - loss: 0.8263 - dice_coef: 0.24 - ETA: 43:09 - loss: 0.8253 - dice_coef: 0.25 - ETA: 43:02 - loss: 0.8252 - dice_coef: 0.25 - ETA: 42:55 - loss: 0.8256 - dice_coef: 0.24 - ETA: 42:48 - loss: 0.8255 - dice_coef: 0.25 - ETA: 42:41 - loss: 0.8251 - dice_coef: 0.25 - ETA: 42:34 - loss: 0.8238 - dice_coef: 0.25 - ETA: 42:27 - loss: 0.8231 - dice_coef: 0.25 - ETA: 42:20 - loss: 0.8235 - dice_coef: 0.25 - ETA: 42:13 - loss: 0.8232 - dice_coef: 0.25 - ETA: 42:06 - loss: 0.8238 - dice_coef: 0.25 - ETA: 41:59 - loss: 0.8234 - dice_coef: 0.25 - ETA: 41:52 - loss: 0.8234 - dice_coef: 0.25 - ETA: 41:45 - loss: 0.8237 - dice_coef: 0.25 - ETA: 41:39 - loss: 0.8242 - dice_coef: 0.25 - ETA: 41:33 - loss: 0.8248 - dice_coef: 0.25 - ETA: 41:27 - loss: 0.8241 - dice_coef: 0.25 - ETA: 41:20 - loss: 0.8248 - dice_coef: 0.25 - ETA: 41:13 - loss: 0.8253 - dice_coef: 0.25 - ETA: 41:06 - loss: 0.8260 - dice_coef: 0.24 - ETA: 40:59 - loss: 0.8257 - dice_coef: 0.24 - ETA: 40:53 - loss: 0.8263 - dice_coef: 0.24 - ETA: 40:46 - loss: 0.8263 - dice_coef: 0.24 - ETA: 40:40 - loss: 0.8253 - dice_coef: 0.25 - ETA: 40:33 - loss: 0.8241 - dice_coef: 0.25 - ETA: 40:26 - loss: 0.8236 - dice_coef: 0.25 - ETA: 40:19 - loss: 0.8235 - dice_coef: 0.25 - ETA: 40:12 - loss: 0.8229 - dice_coef: 0.25 - ETA: 40:05 - loss: 0.8229 - dice_coef: 0.25 - ETA: 39:58 - loss: 0.8230 - dice_coef: 0.25 - ETA: 39:51 - loss: 0.8234 - dice_coef: 0.25 - ETA: 39:44 - loss: 0.8234 - dice_coef: 0.25 - ETA: 39:38 - loss: 0.8234 - dice_coef: 0.25 - ETA: 39:31 - loss: 0.8231 - dice_coef: 0.25 - ETA: 39:24 - loss: 0.8223 - dice_coef: 0.25 - ETA: 39:17 - loss: 0.8222 - dice_coef: 0.25 - ETA: 39:10 - loss: 0.8220 - dice_coef: 0.25 - ETA: 39:03 - loss: 0.8219 - dice_coef: 0.25 - ETA: 38:56 - loss: 0.8211 - dice_coef: 0.25 - ETA: 38:49 - loss: 0.8211 - dice_coef: 0.25 - ETA: 38:42 - loss: 0.8212 - dice_coef: 0.25 - ETA: 38:35 - loss: 0.8212 - dice_coef: 0.25 - ETA: 38:28 - loss: 0.8205 - dice_coef: 0.25 - ETA: 38:21 - loss: 0.8204 - dice_coef: 0.25 - ETA: 38:14 - loss: 0.8195 - dice_coef: 0.25 - ETA: 38:06 - loss: 0.8184 - dice_coef: 0.25 - ETA: 37:59 - loss: 0.8190 - dice_coef: 0.25 - ETA: 37:52 - loss: 0.8187 - dice_coef: 0.25 - ETA: 37:45 - loss: 0.8191 - dice_coef: 0.25 - ETA: 37:38 - loss: 0.8189 - dice_coef: 0.25 - ETA: 37:31 - loss: 0.8183 - dice_coef: 0.25 - ETA: 37:24 - loss: 0.8188 - dice_coef: 0.25 - ETA: 37:17 - loss: 0.8182 - dice_coef: 0.25 - ETA: 37:10 - loss: 0.8175 - dice_coef: 0.25 - ETA: 37:03 - loss: 0.8171 - dice_coef: 0.25 - ETA: 36:56 - loss: 0.8168 - dice_coef: 0.25 - ETA: 36:49 - loss: 0.8158 - dice_coef: 0.25 - ETA: 36:41 - loss: 0.8156 - dice_coef: 0.25 - ETA: 36:34 - loss: 0.8156 - dice_coef: 0.25 - ETA: 36:26 - loss: 0.8157 - dice_coef: 0.25 - ETA: 36:19 - loss: 0.8156 - dice_coef: 0.25 - ETA: 36:12 - loss: 0.8160 - dice_coef: 0.25 - ETA: 36:04 - loss: 0.8165 - dice_coef: 0.25 - ETA: 35:57 - loss: 0.8171 - dice_coef: 0.25 - ETA: 35:49 - loss: 0.8175 - dice_coef: 0.25 - ETA: 35:42 - loss: 0.8173 - dice_coef: 0.25 - ETA: 35:34 - loss: 0.8168 - dice_coef: 0.25 - ETA: 35:27 - loss: 0.8165 - dice_coef: 0.25 - ETA: 35:19 - loss: 0.8162 - dice_coef: 0.25 - ETA: 35:12 - loss: 0.8156 - dice_coef: 0.25 - ETA: 35:04 - loss: 0.8156 - dice_coef: 0.25 - ETA: 34:57 - loss: 0.8155 - dice_coef: 0.25 - ETA: 34:49 - loss: 0.8159 - dice_coef: 0.25 - ETA: 34:42 - loss: 0.8159 - dice_coef: 0.25 - ETA: 34:34 - loss: 0.8156 - dice_coef: 0.25 - ETA: 34:27 - loss: 0.8156 - dice_coef: 0.25 - ETA: 34:20 - loss: 0.8158 - dice_coef: 0.25 - ETA: 34:12 - loss: 0.8150 - dice_coef: 0.25 - ETA: 34:05 - loss: 0.8152 - dice_coef: 0.25 - ETA: 33:57 - loss: 0.8151 - dice_coef: 0.25 - ETA: 33:49 - loss: 0.8144 - dice_coef: 0.25 - ETA: 33:42 - loss: 0.8136 - dice_coef: 0.26 - ETA: 33:34 - loss: 0.8125 - dice_coef: 0.26 - ETA: 33:27 - loss: 0.8118 - dice_coef: 0.26 - ETA: 33:20 - loss: 0.8114 - dice_coef: 0.26 - ETA: 33:12 - loss: 0.8109 - dice_coef: 0.26 - ETA: 33:05 - loss: 0.8110 - dice_coef: 0.26 - ETA: 32:57 - loss: 0.8106 - dice_coef: 0.26 - ETA: 32:50 - loss: 0.8104 - dice_coef: 0.26 - ETA: 32:42 - loss: 0.8108 - dice_coef: 0.26 - ETA: 32:35 - loss: 0.8103 - dice_coef: 0.26 - ETA: 32:27 - loss: 0.8106 - dice_coef: 0.26 - ETA: 32:20 - loss: 0.8108 - dice_coef: 0.26 - ETA: 32:12 - loss: 0.8105 - dice_coef: 0.26 - ETA: 32:05 - loss: 0.8106 - dice_coef: 0.26 - ETA: 31:57 - loss: 0.8103 - dice_coef: 0.26 - ETA: 31:49 - loss: 0.8099 - dice_coef: 0.26 - ETA: 31:42 - loss: 0.8092 - dice_coef: 0.26 - ETA: 31:34 - loss: 0.8092 - dice_coef: 0.26 - ETA: 31:27 - loss: 0.8092 - dice_coef: 0.26 - ETA: 31:19 - loss: 0.8091 - dice_coef: 0.26 - ETA: 31:12 - loss: 0.8095 - dice_coef: 0.26 - ETA: 31:04 - loss: 0.8093 - dice_coef: 0.26 - ETA: 30:57 - loss: 0.8093 - dice_coef: 0.26 - ETA: 30:49 - loss: 0.8083 - dice_coef: 0.26 - ETA: 30:42 - loss: 0.8085 - dice_coef: 0.26 - ETA: 30:34 - loss: 0.8081 - dice_coef: 0.26 - ETA: 30:26 - loss: 0.8074 - dice_coef: 0.26 - ETA: 30:19 - loss: 0.8068 - dice_coef: 0.26 - ETA: 30:11 - loss: 0.8061 - dice_coef: 0.26 - ETA: 30:04 - loss: 0.8060 - dice_coef: 0.26 - ETA: 29:56 - loss: 0.8061 - dice_coef: 0.26 - ETA: 29:48 - loss: 0.8065 - dice_coef: 0.26 - ETA: 29:41 - loss: 0.8058 - dice_coef: 0.26 - ETA: 29:33 - loss: 0.8056 - dice_coef: 0.26 - ETA: 29:25 - loss: 0.8050 - dice_coef: 0.26 - ETA: 29:17 - loss: 0.8052 - dice_coef: 0.26 - ETA: 29:10 - loss: 0.8048 - dice_coef: 0.26 - ETA: 29:03 - loss: 0.8047 - dice_coef: 0.26 - ETA: 28:55 - loss: 0.8042 - dice_coef: 0.26 - ETA: 28:47 - loss: 0.8042 - dice_coef: 0.26 - ETA: 28:40 - loss: 0.8045 - dice_coef: 0.26 - ETA: 28:32 - loss: 0.8045 - dice_coef: 0.26 - ETA: 28:25 - loss: 0.8047 - dice_coef: 0.26 - ETA: 28:17 - loss: 0.8046 - dice_coef: 0.26 - ETA: 28:10 - loss: 0.8043 - dice_coef: 0.26 - ETA: 28:02 - loss: 0.8041 - dice_coef: 0.26 - ETA: 27:55 - loss: 0.8033 - dice_coef: 0.26 - ETA: 27:47 - loss: 0.8036 - dice_coef: 0.26 - ETA: 27:39 - loss: 0.8033 - dice_coef: 0.26 - ETA: 27:31 - loss: 0.8037 - dice_coef: 0.26 - ETA: 27:23 - loss: 0.8034 - dice_coef: 0.26 - ETA: 27:16 - loss: 0.8034 - dice_coef: 0.26 - ETA: 27:08 - loss: 0.8037 - dice_coef: 0.26 - ETA: 27:01 - loss: 0.8035 - dice_coef: 0.26 - ETA: 26:53 - loss: 0.8038 - dice_coef: 0.26 - ETA: 26:45 - loss: 0.8036 - dice_coef: 0.26 - ETA: 26:38 - loss: 0.8034 - dice_coef: 0.26 - ETA: 26:30 - loss: 0.8031 - dice_coef: 0.26 - ETA: 26:22 - loss: 0.8028 - dice_coef: 0.26 - ETA: 26:14 - loss: 0.8029 - dice_coef: 0.26 - ETA: 26:07 - loss: 0.8023 - dice_coef: 0.26 - ETA: 25:59 - loss: 0.8020 - dice_coef: 0.27 - ETA: 25:51 - loss: 0.8017 - dice_coef: 0.27 - ETA: 25:44 - loss: 0.8017 - dice_coef: 0.27 - ETA: 25:36 - loss: 0.8008 - dice_coef: 0.27 - ETA: 25:28 - loss: 0.8007 - dice_coef: 0.27 - ETA: 25:21 - loss: 0.8007 - dice_coef: 0.27 - ETA: 25:13 - loss: 0.8004 - dice_coef: 0.27 - ETA: 25:05 - loss: 0.8001 - dice_coef: 0.27 - ETA: 24:57 - loss: 0.7999 - dice_coef: 0.27 - ETA: 24:49 - loss: 0.7997 - dice_coef: 0.27 - ETA: 24:41 - loss: 0.8002 - dice_coef: 0.27 - ETA: 24:33 - loss: 0.7999 - dice_coef: 0.27 - ETA: 24:26 - loss: 0.7994 - dice_coef: 0.27 - ETA: 24:18 - loss: 0.7996 - dice_coef: 0.27 - ETA: 24:10 - loss: 0.7994 - dice_coef: 0.27 - ETA: 24:03 - loss: 0.7996 - dice_coef: 0.2724"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/708 [============================>.] - ETA: 23:55 - loss: 0.7997 - dice_coef: 0.27 - ETA: 23:47 - loss: 0.8000 - dice_coef: 0.27 - ETA: 23:40 - loss: 0.7994 - dice_coef: 0.27 - ETA: 23:32 - loss: 0.7994 - dice_coef: 0.27 - ETA: 23:24 - loss: 0.7992 - dice_coef: 0.27 - ETA: 23:17 - loss: 0.7990 - dice_coef: 0.27 - ETA: 23:09 - loss: 0.7990 - dice_coef: 0.27 - ETA: 23:01 - loss: 0.7991 - dice_coef: 0.27 - ETA: 22:53 - loss: 0.7994 - dice_coef: 0.27 - ETA: 22:46 - loss: 0.7993 - dice_coef: 0.27 - ETA: 22:38 - loss: 0.7987 - dice_coef: 0.27 - ETA: 22:30 - loss: 0.7983 - dice_coef: 0.27 - ETA: 22:23 - loss: 0.7980 - dice_coef: 0.27 - ETA: 22:15 - loss: 0.7972 - dice_coef: 0.27 - ETA: 22:07 - loss: 0.7964 - dice_coef: 0.27 - ETA: 22:00 - loss: 0.7960 - dice_coef: 0.27 - ETA: 21:52 - loss: 0.7965 - dice_coef: 0.27 - ETA: 21:44 - loss: 0.7967 - dice_coef: 0.27 - ETA: 21:37 - loss: 0.7967 - dice_coef: 0.27 - ETA: 21:29 - loss: 0.7969 - dice_coef: 0.27 - ETA: 21:21 - loss: 0.7972 - dice_coef: 0.27 - ETA: 21:13 - loss: 0.7972 - dice_coef: 0.27 - ETA: 21:06 - loss: 0.7969 - dice_coef: 0.27 - ETA: 20:58 - loss: 0.7965 - dice_coef: 0.27 - ETA: 20:50 - loss: 0.7960 - dice_coef: 0.27 - ETA: 20:43 - loss: 0.7956 - dice_coef: 0.27 - ETA: 20:35 - loss: 0.7957 - dice_coef: 0.27 - ETA: 20:27 - loss: 0.7949 - dice_coef: 0.27 - ETA: 20:20 - loss: 0.7952 - dice_coef: 0.27 - ETA: 20:12 - loss: 0.7954 - dice_coef: 0.27 - ETA: 20:04 - loss: 0.7944 - dice_coef: 0.27 - ETA: 19:57 - loss: 0.7944 - dice_coef: 0.27 - ETA: 19:49 - loss: 0.7939 - dice_coef: 0.27 - ETA: 19:41 - loss: 0.7936 - dice_coef: 0.27 - ETA: 19:34 - loss: 0.7933 - dice_coef: 0.27 - ETA: 19:26 - loss: 0.7936 - dice_coef: 0.27 - ETA: 19:18 - loss: 0.7935 - dice_coef: 0.27 - ETA: 19:10 - loss: 0.7928 - dice_coef: 0.27 - ETA: 19:03 - loss: 0.7928 - dice_coef: 0.27 - ETA: 18:55 - loss: 0.7931 - dice_coef: 0.27 - ETA: 18:47 - loss: 0.7931 - dice_coef: 0.27 - ETA: 18:39 - loss: 0.7930 - dice_coef: 0.27 - ETA: 18:31 - loss: 0.7923 - dice_coef: 0.27 - ETA: 18:24 - loss: 0.7924 - dice_coef: 0.27 - ETA: 18:16 - loss: 0.7920 - dice_coef: 0.27 - ETA: 18:08 - loss: 0.7919 - dice_coef: 0.27 - ETA: 18:00 - loss: 0.7915 - dice_coef: 0.27 - ETA: 17:52 - loss: 0.7915 - dice_coef: 0.27 - ETA: 17:44 - loss: 0.7914 - dice_coef: 0.27 - ETA: 17:36 - loss: 0.7910 - dice_coef: 0.28 - ETA: 17:28 - loss: 0.7907 - dice_coef: 0.28 - ETA: 17:20 - loss: 0.7906 - dice_coef: 0.28 - ETA: 17:12 - loss: 0.7906 - dice_coef: 0.28 - ETA: 17:04 - loss: 0.7906 - dice_coef: 0.28 - ETA: 16:56 - loss: 0.7908 - dice_coef: 0.28 - ETA: 16:48 - loss: 0.7911 - dice_coef: 0.27 - ETA: 16:40 - loss: 0.7916 - dice_coef: 0.27 - ETA: 16:32 - loss: 0.7916 - dice_coef: 0.27 - ETA: 16:24 - loss: 0.7915 - dice_coef: 0.27 - ETA: 16:16 - loss: 0.7914 - dice_coef: 0.27 - ETA: 16:09 - loss: 0.7917 - dice_coef: 0.27 - ETA: 16:01 - loss: 0.7918 - dice_coef: 0.27 - ETA: 15:53 - loss: 0.7918 - dice_coef: 0.27 - ETA: 15:45 - loss: 0.7915 - dice_coef: 0.27 - ETA: 15:37 - loss: 0.7911 - dice_coef: 0.28 - ETA: 15:29 - loss: 0.7914 - dice_coef: 0.27 - ETA: 15:21 - loss: 0.7915 - dice_coef: 0.27 - ETA: 15:13 - loss: 0.7912 - dice_coef: 0.28 - ETA: 15:05 - loss: 0.7906 - dice_coef: 0.28 - ETA: 14:57 - loss: 0.7903 - dice_coef: 0.28 - ETA: 14:49 - loss: 0.7897 - dice_coef: 0.28 - ETA: 14:41 - loss: 0.7898 - dice_coef: 0.28 - ETA: 14:33 - loss: 0.7896 - dice_coef: 0.28 - ETA: 14:25 - loss: 0.7896 - dice_coef: 0.28 - ETA: 14:17 - loss: 0.7893 - dice_coef: 0.28 - ETA: 14:09 - loss: 0.7890 - dice_coef: 0.28 - ETA: 14:02 - loss: 0.7887 - dice_coef: 0.28 - ETA: 13:54 - loss: 0.7888 - dice_coef: 0.28 - ETA: 13:46 - loss: 0.7884 - dice_coef: 0.28 - ETA: 13:38 - loss: 0.7877 - dice_coef: 0.28 - ETA: 13:30 - loss: 0.7880 - dice_coef: 0.28 - ETA: 13:22 - loss: 0.7874 - dice_coef: 0.28 - ETA: 13:14 - loss: 0.7874 - dice_coef: 0.28 - ETA: 13:06 - loss: 0.7878 - dice_coef: 0.28 - ETA: 12:58 - loss: 0.7878 - dice_coef: 0.28 - ETA: 12:50 - loss: 0.7879 - dice_coef: 0.28 - ETA: 12:42 - loss: 0.7879 - dice_coef: 0.28 - ETA: 12:34 - loss: 0.7877 - dice_coef: 0.28 - ETA: 12:26 - loss: 0.7877 - dice_coef: 0.28 - ETA: 12:18 - loss: 0.7870 - dice_coef: 0.28 - ETA: 12:11 - loss: 0.7868 - dice_coef: 0.28 - ETA: 12:03 - loss: 0.7869 - dice_coef: 0.28 - ETA: 11:55 - loss: 0.7869 - dice_coef: 0.28 - ETA: 11:47 - loss: 0.7865 - dice_coef: 0.28 - ETA: 11:39 - loss: 0.7864 - dice_coef: 0.28 - ETA: 11:31 - loss: 0.7859 - dice_coef: 0.28 - ETA: 11:23 - loss: 0.7855 - dice_coef: 0.28 - ETA: 11:15 - loss: 0.7849 - dice_coef: 0.28 - ETA: 11:07 - loss: 0.7851 - dice_coef: 0.28 - ETA: 11:00 - loss: 0.7847 - dice_coef: 0.28 - ETA: 10:52 - loss: 0.7847 - dice_coef: 0.28 - ETA: 10:44 - loss: 0.7846 - dice_coef: 0.28 - ETA: 10:36 - loss: 0.7843 - dice_coef: 0.28 - ETA: 10:28 - loss: 0.7834 - dice_coef: 0.28 - ETA: 10:20 - loss: 0.7834 - dice_coef: 0.28 - ETA: 10:12 - loss: 0.7836 - dice_coef: 0.28 - ETA: 10:05 - loss: 0.7835 - dice_coef: 0.28 - ETA: 9:57 - loss: 0.7827 - dice_coef: 0.2882 - ETA: 9:49 - loss: 0.7825 - dice_coef: 0.288 - ETA: 9:41 - loss: 0.7826 - dice_coef: 0.288 - ETA: 9:33 - loss: 0.7823 - dice_coef: 0.288 - ETA: 9:25 - loss: 0.7821 - dice_coef: 0.288 - ETA: 9:17 - loss: 0.7824 - dice_coef: 0.288 - ETA: 9:10 - loss: 0.7827 - dice_coef: 0.287 - ETA: 9:02 - loss: 0.7823 - dice_coef: 0.288 - ETA: 8:54 - loss: 0.7825 - dice_coef: 0.288 - ETA: 8:46 - loss: 0.7824 - dice_coef: 0.288 - ETA: 8:38 - loss: 0.7823 - dice_coef: 0.288 - ETA: 8:31 - loss: 0.7818 - dice_coef: 0.288 - ETA: 8:23 - loss: 0.7816 - dice_coef: 0.289 - ETA: 8:15 - loss: 0.7815 - dice_coef: 0.289 - ETA: 8:07 - loss: 0.7814 - dice_coef: 0.289 - ETA: 7:59 - loss: 0.7810 - dice_coef: 0.289 - ETA: 7:51 - loss: 0.7807 - dice_coef: 0.290 - ETA: 7:44 - loss: 0.7808 - dice_coef: 0.290 - ETA: 7:36 - loss: 0.7809 - dice_coef: 0.289 - ETA: 7:28 - loss: 0.7805 - dice_coef: 0.290 - ETA: 7:20 - loss: 0.7801 - dice_coef: 0.290 - ETA: 7:12 - loss: 0.7798 - dice_coef: 0.290 - ETA: 7:05 - loss: 0.7799 - dice_coef: 0.290 - ETA: 6:57 - loss: 0.7796 - dice_coef: 0.291 - ETA: 6:49 - loss: 0.7789 - dice_coef: 0.291 - ETA: 6:41 - loss: 0.7789 - dice_coef: 0.291 - ETA: 6:34 - loss: 0.7787 - dice_coef: 0.292 - ETA: 6:26 - loss: 0.7784 - dice_coef: 0.292 - ETA: 6:18 - loss: 0.7781 - dice_coef: 0.292 - ETA: 6:10 - loss: 0.7782 - dice_coef: 0.292 - ETA: 6:02 - loss: 0.7783 - dice_coef: 0.292 - ETA: 5:55 - loss: 0.7782 - dice_coef: 0.292 - ETA: 5:47 - loss: 0.7785 - dice_coef: 0.292 - ETA: 5:39 - loss: 0.7785 - dice_coef: 0.292 - ETA: 5:32 - loss: 0.7784 - dice_coef: 0.292 - ETA: 5:24 - loss: 0.7786 - dice_coef: 0.292 - ETA: 5:16 - loss: 0.7779 - dice_coef: 0.292 - ETA: 5:08 - loss: 0.7777 - dice_coef: 0.293 - ETA: 5:01 - loss: 0.7772 - dice_coef: 0.293 - ETA: 4:53 - loss: 0.7771 - dice_coef: 0.293 - ETA: 4:45 - loss: 0.7771 - dice_coef: 0.293 - ETA: 4:37 - loss: 0.7774 - dice_coef: 0.293 - ETA: 4:30 - loss: 0.7773 - dice_coef: 0.293 - ETA: 4:22 - loss: 0.7767 - dice_coef: 0.294 - ETA: 4:14 - loss: 0.7764 - dice_coef: 0.294 - ETA: 4:06 - loss: 0.7761 - dice_coef: 0.295 - ETA: 3:59 - loss: 0.7755 - dice_coef: 0.295 - ETA: 3:51 - loss: 0.7754 - dice_coef: 0.295 - ETA: 3:43 - loss: 0.7755 - dice_coef: 0.295 - ETA: 3:36 - loss: 0.7753 - dice_coef: 0.295 - ETA: 3:28 - loss: 0.7748 - dice_coef: 0.296 - ETA: 3:20 - loss: 0.7745 - dice_coef: 0.296 - ETA: 3:12 - loss: 0.7739 - dice_coef: 0.297 - ETA: 3:05 - loss: 0.7741 - dice_coef: 0.296 - ETA: 2:57 - loss: 0.7738 - dice_coef: 0.297 - ETA: 2:49 - loss: 0.7737 - dice_coef: 0.297 - ETA: 2:42 - loss: 0.7734 - dice_coef: 0.297 - ETA: 2:34 - loss: 0.7729 - dice_coef: 0.297 - ETA: 2:26 - loss: 0.7727 - dice_coef: 0.298 - ETA: 2:18 - loss: 0.7725 - dice_coef: 0.298 - ETA: 2:11 - loss: 0.7726 - dice_coef: 0.298 - ETA: 2:03 - loss: 0.7723 - dice_coef: 0.298 - ETA: 1:55 - loss: 0.7722 - dice_coef: 0.298 - ETA: 1:48 - loss: 0.7723 - dice_coef: 0.298 - ETA: 1:40 - loss: 0.7723 - dice_coef: 0.298 - ETA: 1:32 - loss: 0.7721 - dice_coef: 0.298 - ETA: 1:24 - loss: 0.7717 - dice_coef: 0.298 - ETA: 1:17 - loss: 0.7717 - dice_coef: 0.298 - ETA: 1:09 - loss: 0.7714 - dice_coef: 0.299 - ETA: 1:01 - loss: 0.7711 - dice_coef: 0.299 - ETA: 53s - loss: 0.7714 - dice_coef: 0.2989 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708/708 [==============================] - ETA: 46s - loss: 0.7717 - dice_coef: 0.29 - ETA: 38s - loss: 0.7719 - dice_coef: 0.29 - ETA: 30s - loss: 0.7721 - dice_coef: 0.29 - ETA: 23s - loss: 0.7725 - dice_coef: 0.29 - ETA: 15s - loss: 0.7722 - dice_coef: 0.29 - ETA: 7s - loss: 0.7725 - dice_coef: 0.2979 - 5842s 8s/step - loss: 0.7727 - dice_coef: 0.2978 - val_loss: 0.7448 - val_dice_coef: 0.3192\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/708 [======>.......................] - ETA: 1:34:16 - loss: 0.6507 - dice_coef: 0.42 - ETA: 1:31:03 - loss: 0.7863 - dice_coef: 0.28 - ETA: 1:30:00 - loss: 0.7706 - dice_coef: 0.30 - ETA: 1:28:49 - loss: 0.7821 - dice_coef: 0.30 - ETA: 1:28:03 - loss: 0.7636 - dice_coef: 0.31 - ETA: 1:27:55 - loss: 0.7884 - dice_coef: 0.28 - ETA: 1:27:30 - loss: 0.8243 - dice_coef: 0.24 - ETA: 1:27:22 - loss: 0.8173 - dice_coef: 0.25 - ETA: 1:27:46 - loss: 0.8271 - dice_coef: 0.24 - ETA: 1:28:04 - loss: 0.8130 - dice_coef: 0.25 - ETA: 1:27:56 - loss: 0.8034 - dice_coef: 0.26 - ETA: 1:27:36 - loss: 0.7903 - dice_coef: 0.27 - ETA: 1:27:39 - loss: 0.7824 - dice_coef: 0.28 - ETA: 1:27:38 - loss: 0.7879 - dice_coef: 0.27 - ETA: 1:27:34 - loss: 0.7887 - dice_coef: 0.27 - ETA: 1:27:20 - loss: 0.8041 - dice_coef: 0.25 - ETA: 1:27:00 - loss: 0.7916 - dice_coef: 0.26 - ETA: 1:26:43 - loss: 0.7754 - dice_coef: 0.28 - ETA: 1:26:38 - loss: 0.7783 - dice_coef: 0.28 - ETA: 1:26:33 - loss: 0.7822 - dice_coef: 0.29 - ETA: 1:26:23 - loss: 0.7777 - dice_coef: 0.29 - ETA: 1:26:17 - loss: 0.7674 - dice_coef: 0.30 - ETA: 1:26:26 - loss: 0.7593 - dice_coef: 0.30 - ETA: 1:26:29 - loss: 0.7475 - dice_coef: 0.31 - ETA: 1:26:18 - loss: 0.7309 - dice_coef: 0.33 - ETA: 1:26:11 - loss: 0.7463 - dice_coef: 0.32 - ETA: 1:26:01 - loss: 0.7387 - dice_coef: 0.33 - ETA: 1:25:50 - loss: 0.7278 - dice_coef: 0.34 - ETA: 1:25:33 - loss: 0.7222 - dice_coef: 0.34 - ETA: 1:25:18 - loss: 0.7268 - dice_coef: 0.34 - ETA: 1:25:07 - loss: 0.7303 - dice_coef: 0.34 - ETA: 1:24:53 - loss: 0.7316 - dice_coef: 0.34 - ETA: 1:24:41 - loss: 0.7293 - dice_coef: 0.34 - ETA: 1:24:31 - loss: 0.7283 - dice_coef: 0.34 - ETA: 1:24:18 - loss: 0.7310 - dice_coef: 0.34 - ETA: 1:24:05 - loss: 0.7226 - dice_coef: 0.35 - ETA: 1:23:52 - loss: 0.7306 - dice_coef: 0.34 - ETA: 1:23:40 - loss: 0.7212 - dice_coef: 0.35 - ETA: 1:23:32 - loss: 0.7179 - dice_coef: 0.35 - ETA: 1:23:24 - loss: 0.7092 - dice_coef: 0.36 - ETA: 1:23:11 - loss: 0.7062 - dice_coef: 0.36 - ETA: 1:23:00 - loss: 0.7022 - dice_coef: 0.37 - ETA: 1:22:51 - loss: 0.6989 - dice_coef: 0.37 - ETA: 1:22:44 - loss: 0.6993 - dice_coef: 0.37 - ETA: 1:22:37 - loss: 0.6951 - dice_coef: 0.37 - ETA: 1:22:27 - loss: 0.7021 - dice_coef: 0.37 - ETA: 1:22:17 - loss: 0.7072 - dice_coef: 0.36 - ETA: 1:22:09 - loss: 0.7137 - dice_coef: 0.35 - ETA: 1:21:58 - loss: 0.7175 - dice_coef: 0.35 - ETA: 1:21:48 - loss: 0.7099 - dice_coef: 0.36 - ETA: 1:21:42 - loss: 0.7129 - dice_coef: 0.35 - ETA: 1:21:35 - loss: 0.7188 - dice_coef: 0.35 - ETA: 1:21:27 - loss: 0.7239 - dice_coef: 0.34 - ETA: 1:21:18 - loss: 0.7252 - dice_coef: 0.34 - ETA: 1:21:07 - loss: 0.7248 - dice_coef: 0.34 - ETA: 1:20:56 - loss: 0.7210 - dice_coef: 0.35 - ETA: 1:20:44 - loss: 0.7160 - dice_coef: 0.35 - ETA: 1:20:33 - loss: 0.7214 - dice_coef: 0.35 - ETA: 1:20:24 - loss: 0.7125 - dice_coef: 0.35 - ETA: 1:20:14 - loss: 0.7115 - dice_coef: 0.36 - ETA: 1:20:06 - loss: 0.7072 - dice_coef: 0.36 - ETA: 1:19:58 - loss: 0.7045 - dice_coef: 0.36 - ETA: 1:19:50 - loss: 0.7020 - dice_coef: 0.36 - ETA: 1:19:41 - loss: 0.7047 - dice_coef: 0.36 - ETA: 1:19:34 - loss: 0.7035 - dice_coef: 0.36 - ETA: 1:19:25 - loss: 0.7005 - dice_coef: 0.37 - ETA: 1:19:17 - loss: 0.7022 - dice_coef: 0.36 - ETA: 1:19:09 - loss: 0.6983 - dice_coef: 0.37 - ETA: 1:19:02 - loss: 0.6994 - dice_coef: 0.37 - ETA: 1:18:55 - loss: 0.6978 - dice_coef: 0.37 - ETA: 1:18:48 - loss: 0.6971 - dice_coef: 0.37 - ETA: 1:18:39 - loss: 0.6932 - dice_coef: 0.37 - ETA: 1:18:30 - loss: 0.6945 - dice_coef: 0.37 - ETA: 1:18:24 - loss: 0.6915 - dice_coef: 0.37 - ETA: 1:18:16 - loss: 0.6898 - dice_coef: 0.38 - ETA: 1:18:08 - loss: 0.6908 - dice_coef: 0.38 - ETA: 1:18:01 - loss: 0.6907 - dice_coef: 0.38 - ETA: 1:17:53 - loss: 0.6890 - dice_coef: 0.38 - ETA: 1:17:45 - loss: 0.6866 - dice_coef: 0.38 - ETA: 1:17:37 - loss: 0.6834 - dice_coef: 0.38 - ETA: 1:17:29 - loss: 0.6830 - dice_coef: 0.38 - ETA: 1:17:20 - loss: 0.6841 - dice_coef: 0.38 - ETA: 1:17:13 - loss: 0.6825 - dice_coef: 0.38 - ETA: 1:17:04 - loss: 0.6787 - dice_coef: 0.39 - ETA: 1:16:56 - loss: 0.6769 - dice_coef: 0.39 - ETA: 1:16:49 - loss: 0.6766 - dice_coef: 0.39 - ETA: 1:16:40 - loss: 0.6747 - dice_coef: 0.39 - ETA: 1:16:32 - loss: 0.6753 - dice_coef: 0.39 - ETA: 1:16:23 - loss: 0.6781 - dice_coef: 0.38 - ETA: 1:16:15 - loss: 0.6774 - dice_coef: 0.39 - ETA: 1:16:06 - loss: 0.6800 - dice_coef: 0.38 - ETA: 1:15:58 - loss: 0.6816 - dice_coef: 0.38 - ETA: 1:15:50 - loss: 0.6820 - dice_coef: 0.38 - ETA: 1:15:41 - loss: 0.6795 - dice_coef: 0.38 - ETA: 1:15:34 - loss: 0.6797 - dice_coef: 0.38 - ETA: 1:15:25 - loss: 0.6797 - dice_coef: 0.38 - ETA: 1:15:18 - loss: 0.6833 - dice_coef: 0.38 - ETA: 1:15:11 - loss: 0.6837 - dice_coef: 0.38 - ETA: 1:15:04 - loss: 0.6847 - dice_coef: 0.38 - ETA: 1:14:57 - loss: 0.6875 - dice_coef: 0.37 - ETA: 1:14:49 - loss: 0.6874 - dice_coef: 0.37 - ETA: 1:14:41 - loss: 0.6903 - dice_coef: 0.37 - ETA: 1:14:35 - loss: 0.6901 - dice_coef: 0.37 - ETA: 1:14:29 - loss: 0.6896 - dice_coef: 0.37 - ETA: 1:14:20 - loss: 0.6888 - dice_coef: 0.37 - ETA: 1:14:13 - loss: 0.6890 - dice_coef: 0.37 - ETA: 1:14:05 - loss: 0.6856 - dice_coef: 0.37 - ETA: 1:13:57 - loss: 0.6869 - dice_coef: 0.37 - ETA: 1:13:50 - loss: 0.6880 - dice_coef: 0.37 - ETA: 1:13:41 - loss: 0.6905 - dice_coef: 0.37 - ETA: 1:13:34 - loss: 0.6907 - dice_coef: 0.37 - ETA: 1:13:27 - loss: 0.6888 - dice_coef: 0.37 - ETA: 1:13:20 - loss: 0.6886 - dice_coef: 0.37 - ETA: 1:13:12 - loss: 0.6892 - dice_coef: 0.37 - ETA: 1:13:05 - loss: 0.6921 - dice_coef: 0.37 - ETA: 1:12:57 - loss: 0.6928 - dice_coef: 0.37 - ETA: 1:12:49 - loss: 0.6947 - dice_coef: 0.36 - ETA: 1:12:42 - loss: 0.6948 - dice_coef: 0.36 - ETA: 1:12:35 - loss: 0.6962 - dice_coef: 0.36 - ETA: 1:12:28 - loss: 0.6994 - dice_coef: 0.36 - ETA: 1:12:21 - loss: 0.6992 - dice_coef: 0.36 - ETA: 1:12:12 - loss: 0.6983 - dice_coef: 0.36 - ETA: 1:12:04 - loss: 0.6983 - dice_coef: 0.36 - ETA: 1:11:56 - loss: 0.6989 - dice_coef: 0.36 - ETA: 1:11:48 - loss: 0.6966 - dice_coef: 0.36 - ETA: 1:11:43 - loss: 0.6949 - dice_coef: 0.36 - ETA: 1:11:35 - loss: 0.6940 - dice_coef: 0.37 - ETA: 1:11:28 - loss: 0.6915 - dice_coef: 0.37 - ETA: 1:11:20 - loss: 0.6903 - dice_coef: 0.37 - ETA: 1:11:12 - loss: 0.6901 - dice_coef: 0.37 - ETA: 1:11:05 - loss: 0.6904 - dice_coef: 0.37 - ETA: 1:10:57 - loss: 0.6934 - dice_coef: 0.37 - ETA: 1:10:49 - loss: 0.6920 - dice_coef: 0.37 - ETA: 1:10:41 - loss: 0.6900 - dice_coef: 0.37 - ETA: 1:10:33 - loss: 0.6903 - dice_coef: 0.37 - ETA: 1:10:25 - loss: 0.6886 - dice_coef: 0.37 - ETA: 1:10:18 - loss: 0.6877 - dice_coef: 0.37 - ETA: 1:10:10 - loss: 0.6901 - dice_coef: 0.37 - ETA: 1:10:03 - loss: 0.6912 - dice_coef: 0.37 - ETA: 1:09:56 - loss: 0.6897 - dice_coef: 0.37 - ETA: 1:09:49 - loss: 0.6895 - dice_coef: 0.37 - ETA: 1:09:42 - loss: 0.6876 - dice_coef: 0.37 - ETA: 1:09:35 - loss: 0.6897 - dice_coef: 0.37 - ETA: 1:09:27 - loss: 0.6874 - dice_coef: 0.37 - ETA: 1:09:20 - loss: 0.6879 - dice_coef: 0.37 - ETA: 1:09:13 - loss: 0.6905 - dice_coef: 0.37 - ETA: 1:09:05 - loss: 0.6895 - dice_coef: 0.37 - ETA: 1:08:58 - loss: 0.6888 - dice_coef: 0.37 - ETA: 1:08:51 - loss: 0.6883 - dice_coef: 0.37 - ETA: 1:08:44 - loss: 0.6905 - dice_coef: 0.37 - ETA: 1:08:36 - loss: 0.6890 - dice_coef: 0.37 - ETA: 1:08:29 - loss: 0.6861 - dice_coef: 0.37 - ETA: 1:08:21 - loss: 0.6853 - dice_coef: 0.37 - ETA: 1:08:13 - loss: 0.6864 - dice_coef: 0.37 - ETA: 1:08:05 - loss: 0.6865 - dice_coef: 0.37 - ETA: 1:07:57 - loss: 0.6859 - dice_coef: 0.37 - ETA: 1:07:50 - loss: 0.6859 - dice_coef: 0.37 - ETA: 1:07:42 - loss: 0.6855 - dice_coef: 0.37 - ETA: 1:07:34 - loss: 0.6841 - dice_coef: 0.37 - ETA: 1:07:27 - loss: 0.6855 - dice_coef: 0.37 - ETA: 1:07:21 - loss: 0.6857 - dice_coef: 0.37 - ETA: 1:07:14 - loss: 0.6853 - dice_coef: 0.37 - ETA: 1:07:07 - loss: 0.6847 - dice_coef: 0.37 - ETA: 1:07:00 - loss: 0.6849 - dice_coef: 0.37 - ETA: 1:06:53 - loss: 0.6835 - dice_coef: 0.37 - ETA: 1:06:45 - loss: 0.6848 - dice_coef: 0.37 - ETA: 1:06:38 - loss: 0.6869 - dice_coef: 0.37 - ETA: 1:06:30 - loss: 0.6889 - dice_coef: 0.37 - ETA: 1:06:22 - loss: 0.6881 - dice_coef: 0.37 - ETA: 1:06:14 - loss: 0.6891 - dice_coef: 0.3741"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346/708 [=============>................] - ETA: 1:06:06 - loss: 0.6907 - dice_coef: 0.37 - ETA: 1:05:59 - loss: 0.6890 - dice_coef: 0.37 - ETA: 1:05:51 - loss: 0.6879 - dice_coef: 0.37 - ETA: 1:05:44 - loss: 0.6898 - dice_coef: 0.37 - ETA: 1:05:36 - loss: 0.6883 - dice_coef: 0.37 - ETA: 1:05:29 - loss: 0.6891 - dice_coef: 0.37 - ETA: 1:05:21 - loss: 0.6876 - dice_coef: 0.37 - ETA: 1:05:14 - loss: 0.6884 - dice_coef: 0.37 - ETA: 1:05:06 - loss: 0.6872 - dice_coef: 0.37 - ETA: 1:04:58 - loss: 0.6882 - dice_coef: 0.37 - ETA: 1:04:50 - loss: 0.6876 - dice_coef: 0.37 - ETA: 1:04:42 - loss: 0.6894 - dice_coef: 0.37 - ETA: 1:04:34 - loss: 0.6887 - dice_coef: 0.37 - ETA: 1:04:27 - loss: 0.6874 - dice_coef: 0.37 - ETA: 1:04:19 - loss: 0.6865 - dice_coef: 0.37 - ETA: 1:04:11 - loss: 0.6864 - dice_coef: 0.37 - ETA: 1:04:04 - loss: 0.6839 - dice_coef: 0.37 - ETA: 1:03:56 - loss: 0.6849 - dice_coef: 0.37 - ETA: 1:03:49 - loss: 0.6839 - dice_coef: 0.37 - ETA: 1:03:41 - loss: 0.6852 - dice_coef: 0.37 - ETA: 1:03:33 - loss: 0.6856 - dice_coef: 0.37 - ETA: 1:03:25 - loss: 0.6843 - dice_coef: 0.37 - ETA: 1:03:18 - loss: 0.6848 - dice_coef: 0.37 - ETA: 1:03:11 - loss: 0.6862 - dice_coef: 0.37 - ETA: 1:03:04 - loss: 0.6854 - dice_coef: 0.37 - ETA: 1:02:56 - loss: 0.6847 - dice_coef: 0.37 - ETA: 1:02:49 - loss: 0.6865 - dice_coef: 0.37 - ETA: 1:02:41 - loss: 0.6870 - dice_coef: 0.37 - ETA: 1:02:34 - loss: 0.6883 - dice_coef: 0.37 - ETA: 1:02:27 - loss: 0.6874 - dice_coef: 0.37 - ETA: 1:02:20 - loss: 0.6876 - dice_coef: 0.37 - ETA: 1:02:12 - loss: 0.6876 - dice_coef: 0.37 - ETA: 1:02:04 - loss: 0.6858 - dice_coef: 0.37 - ETA: 1:01:56 - loss: 0.6869 - dice_coef: 0.37 - ETA: 1:01:49 - loss: 0.6883 - dice_coef: 0.37 - ETA: 1:01:41 - loss: 0.6895 - dice_coef: 0.37 - ETA: 1:01:34 - loss: 0.6899 - dice_coef: 0.37 - ETA: 1:01:26 - loss: 0.6892 - dice_coef: 0.37 - ETA: 1:01:18 - loss: 0.6896 - dice_coef: 0.37 - ETA: 1:01:10 - loss: 0.6894 - dice_coef: 0.37 - ETA: 1:01:02 - loss: 0.6913 - dice_coef: 0.37 - ETA: 1:00:54 - loss: 0.6907 - dice_coef: 0.37 - ETA: 1:00:47 - loss: 0.6913 - dice_coef: 0.37 - ETA: 1:00:39 - loss: 0.6923 - dice_coef: 0.37 - ETA: 1:00:32 - loss: 0.6939 - dice_coef: 0.36 - ETA: 1:00:24 - loss: 0.6952 - dice_coef: 0.36 - ETA: 1:00:16 - loss: 0.6962 - dice_coef: 0.36 - ETA: 1:00:09 - loss: 0.6960 - dice_coef: 0.36 - ETA: 1:00:01 - loss: 0.6964 - dice_coef: 0.36 - ETA: 59:53 - loss: 0.6974 - dice_coef: 0.3653 - ETA: 59:46 - loss: 0.6990 - dice_coef: 0.36 - ETA: 59:39 - loss: 0.6987 - dice_coef: 0.36 - ETA: 59:31 - loss: 0.6987 - dice_coef: 0.36 - ETA: 59:24 - loss: 0.6977 - dice_coef: 0.36 - ETA: 59:17 - loss: 0.6975 - dice_coef: 0.36 - ETA: 59:09 - loss: 0.6966 - dice_coef: 0.36 - ETA: 59:02 - loss: 0.6975 - dice_coef: 0.36 - ETA: 58:55 - loss: 0.6969 - dice_coef: 0.36 - ETA: 58:48 - loss: 0.6987 - dice_coef: 0.36 - ETA: 58:40 - loss: 0.6970 - dice_coef: 0.36 - ETA: 58:33 - loss: 0.6976 - dice_coef: 0.36 - ETA: 58:25 - loss: 0.6967 - dice_coef: 0.36 - ETA: 58:18 - loss: 0.6963 - dice_coef: 0.36 - ETA: 58:11 - loss: 0.6956 - dice_coef: 0.36 - ETA: 58:03 - loss: 0.6966 - dice_coef: 0.36 - ETA: 57:56 - loss: 0.6962 - dice_coef: 0.36 - ETA: 57:48 - loss: 0.6947 - dice_coef: 0.36 - ETA: 57:41 - loss: 0.6956 - dice_coef: 0.36 - ETA: 57:34 - loss: 0.6969 - dice_coef: 0.36 - ETA: 57:27 - loss: 0.6982 - dice_coef: 0.36 - ETA: 57:19 - loss: 0.6975 - dice_coef: 0.36 - ETA: 57:12 - loss: 0.6971 - dice_coef: 0.36 - ETA: 57:05 - loss: 0.6977 - dice_coef: 0.36 - ETA: 56:57 - loss: 0.6982 - dice_coef: 0.36 - ETA: 56:50 - loss: 0.6969 - dice_coef: 0.36 - ETA: 56:42 - loss: 0.6970 - dice_coef: 0.36 - ETA: 56:35 - loss: 0.6972 - dice_coef: 0.36 - ETA: 56:28 - loss: 0.6988 - dice_coef: 0.36 - ETA: 56:20 - loss: 0.6994 - dice_coef: 0.36 - ETA: 56:12 - loss: 0.7003 - dice_coef: 0.36 - ETA: 56:06 - loss: 0.6998 - dice_coef: 0.36 - ETA: 55:58 - loss: 0.6991 - dice_coef: 0.36 - ETA: 55:51 - loss: 0.7003 - dice_coef: 0.36 - ETA: 55:43 - loss: 0.6998 - dice_coef: 0.36 - ETA: 55:36 - loss: 0.6988 - dice_coef: 0.36 - ETA: 55:28 - loss: 0.6978 - dice_coef: 0.36 - ETA: 55:21 - loss: 0.6967 - dice_coef: 0.36 - ETA: 55:14 - loss: 0.6962 - dice_coef: 0.36 - ETA: 55:07 - loss: 0.6963 - dice_coef: 0.36 - ETA: 54:59 - loss: 0.6953 - dice_coef: 0.36 - ETA: 54:51 - loss: 0.6964 - dice_coef: 0.36 - ETA: 54:44 - loss: 0.6978 - dice_coef: 0.36 - ETA: 54:36 - loss: 0.6985 - dice_coef: 0.36 - ETA: 54:29 - loss: 0.6982 - dice_coef: 0.36 - ETA: 54:21 - loss: 0.6992 - dice_coef: 0.36 - ETA: 54:14 - loss: 0.6995 - dice_coef: 0.36 - ETA: 54:07 - loss: 0.7003 - dice_coef: 0.36 - ETA: 54:00 - loss: 0.7006 - dice_coef: 0.36 - ETA: 53:53 - loss: 0.7017 - dice_coef: 0.36 - ETA: 53:46 - loss: 0.7026 - dice_coef: 0.36 - ETA: 53:38 - loss: 0.7037 - dice_coef: 0.36 - ETA: 53:31 - loss: 0.7030 - dice_coef: 0.36 - ETA: 53:23 - loss: 0.7026 - dice_coef: 0.36 - ETA: 53:16 - loss: 0.7038 - dice_coef: 0.36 - ETA: 53:08 - loss: 0.7033 - dice_coef: 0.36 - ETA: 53:01 - loss: 0.7035 - dice_coef: 0.36 - ETA: 52:54 - loss: 0.7026 - dice_coef: 0.36 - ETA: 52:46 - loss: 0.7012 - dice_coef: 0.36 - ETA: 52:39 - loss: 0.7002 - dice_coef: 0.36 - ETA: 52:32 - loss: 0.7014 - dice_coef: 0.36 - ETA: 52:24 - loss: 0.7026 - dice_coef: 0.36 - ETA: 52:16 - loss: 0.7037 - dice_coef: 0.35 - ETA: 52:09 - loss: 0.7050 - dice_coef: 0.35 - ETA: 52:02 - loss: 0.7061 - dice_coef: 0.35 - ETA: 51:54 - loss: 0.7071 - dice_coef: 0.35 - ETA: 51:47 - loss: 0.7079 - dice_coef: 0.35 - ETA: 51:40 - loss: 0.7078 - dice_coef: 0.35 - ETA: 51:32 - loss: 0.7070 - dice_coef: 0.35 - ETA: 51:25 - loss: 0.7071 - dice_coef: 0.35 - ETA: 51:17 - loss: 0.7069 - dice_coef: 0.35 - ETA: 51:10 - loss: 0.7079 - dice_coef: 0.35 - ETA: 51:03 - loss: 0.7077 - dice_coef: 0.35 - ETA: 50:55 - loss: 0.7076 - dice_coef: 0.35 - ETA: 50:48 - loss: 0.7081 - dice_coef: 0.35 - ETA: 50:40 - loss: 0.7083 - dice_coef: 0.35 - ETA: 50:33 - loss: 0.7068 - dice_coef: 0.35 - ETA: 50:25 - loss: 0.7064 - dice_coef: 0.35 - ETA: 50:18 - loss: 0.7065 - dice_coef: 0.35 - ETA: 50:10 - loss: 0.7049 - dice_coef: 0.35 - ETA: 50:03 - loss: 0.7057 - dice_coef: 0.35 - ETA: 49:56 - loss: 0.7056 - dice_coef: 0.35 - ETA: 49:48 - loss: 0.7046 - dice_coef: 0.35 - ETA: 49:41 - loss: 0.7035 - dice_coef: 0.36 - ETA: 49:34 - loss: 0.7042 - dice_coef: 0.35 - ETA: 49:26 - loss: 0.7039 - dice_coef: 0.35 - ETA: 49:18 - loss: 0.7045 - dice_coef: 0.35 - ETA: 49:10 - loss: 0.7041 - dice_coef: 0.35 - ETA: 49:04 - loss: 0.7043 - dice_coef: 0.35 - ETA: 48:57 - loss: 0.7050 - dice_coef: 0.35 - ETA: 48:50 - loss: 0.7050 - dice_coef: 0.35 - ETA: 48:43 - loss: 0.7050 - dice_coef: 0.35 - ETA: 48:36 - loss: 0.7048 - dice_coef: 0.35 - ETA: 48:28 - loss: 0.7043 - dice_coef: 0.35 - ETA: 48:21 - loss: 0.7038 - dice_coef: 0.36 - ETA: 48:14 - loss: 0.7048 - dice_coef: 0.35 - ETA: 48:06 - loss: 0.7039 - dice_coef: 0.35 - ETA: 47:58 - loss: 0.7037 - dice_coef: 0.36 - ETA: 47:51 - loss: 0.7035 - dice_coef: 0.36 - ETA: 47:43 - loss: 0.7023 - dice_coef: 0.36 - ETA: 47:36 - loss: 0.7020 - dice_coef: 0.36 - ETA: 47:29 - loss: 0.7029 - dice_coef: 0.36 - ETA: 47:21 - loss: 0.7025 - dice_coef: 0.36 - ETA: 47:14 - loss: 0.7033 - dice_coef: 0.36 - ETA: 47:06 - loss: 0.7033 - dice_coef: 0.36 - ETA: 46:59 - loss: 0.7033 - dice_coef: 0.36 - ETA: 46:52 - loss: 0.7027 - dice_coef: 0.36 - ETA: 46:44 - loss: 0.7035 - dice_coef: 0.35 - ETA: 46:37 - loss: 0.7039 - dice_coef: 0.35 - ETA: 46:29 - loss: 0.7038 - dice_coef: 0.35 - ETA: 46:22 - loss: 0.7031 - dice_coef: 0.36 - ETA: 46:14 - loss: 0.7023 - dice_coef: 0.36 - ETA: 46:07 - loss: 0.7013 - dice_coef: 0.36 - ETA: 46:00 - loss: 0.7010 - dice_coef: 0.36 - ETA: 45:52 - loss: 0.7006 - dice_coef: 0.36 - ETA: 45:45 - loss: 0.7005 - dice_coef: 0.36 - ETA: 45:37 - loss: 0.7013 - dice_coef: 0.36 - ETA: 45:30 - loss: 0.7006 - dice_coef: 0.36 - ETA: 45:22 - loss: 0.7000 - dice_coef: 0.36 - ETA: 45:15 - loss: 0.6988 - dice_coef: 0.36 - ETA: 45:07 - loss: 0.6982 - dice_coef: 0.36 - ETA: 45:00 - loss: 0.6977 - dice_coef: 0.36 - ETA: 44:52 - loss: 0.6978 - dice_coef: 0.36 - ETA: 44:45 - loss: 0.6976 - dice_coef: 0.36 - ETA: 44:37 - loss: 0.6977 - dice_coef: 0.36 - ETA: 44:30 - loss: 0.6978 - dice_coef: 0.36 - ETA: 44:23 - loss: 0.6969 - dice_coef: 0.3661"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524/708 [=====================>........] - ETA: 44:15 - loss: 0.6960 - dice_coef: 0.36 - ETA: 44:08 - loss: 0.6970 - dice_coef: 0.36 - ETA: 44:01 - loss: 0.6970 - dice_coef: 0.36 - ETA: 43:53 - loss: 0.6961 - dice_coef: 0.36 - ETA: 43:46 - loss: 0.6967 - dice_coef: 0.36 - ETA: 43:39 - loss: 0.6967 - dice_coef: 0.36 - ETA: 43:31 - loss: 0.6964 - dice_coef: 0.36 - ETA: 43:24 - loss: 0.6963 - dice_coef: 0.36 - ETA: 43:16 - loss: 0.6955 - dice_coef: 0.36 - ETA: 43:09 - loss: 0.6948 - dice_coef: 0.36 - ETA: 43:01 - loss: 0.6950 - dice_coef: 0.36 - ETA: 42:54 - loss: 0.6948 - dice_coef: 0.36 - ETA: 42:46 - loss: 0.6949 - dice_coef: 0.36 - ETA: 42:39 - loss: 0.6945 - dice_coef: 0.36 - ETA: 42:31 - loss: 0.6947 - dice_coef: 0.36 - ETA: 42:24 - loss: 0.6939 - dice_coef: 0.36 - ETA: 42:17 - loss: 0.6942 - dice_coef: 0.36 - ETA: 42:10 - loss: 0.6950 - dice_coef: 0.36 - ETA: 42:02 - loss: 0.6944 - dice_coef: 0.36 - ETA: 41:55 - loss: 0.6942 - dice_coef: 0.36 - ETA: 41:47 - loss: 0.6945 - dice_coef: 0.36 - ETA: 41:40 - loss: 0.6936 - dice_coef: 0.36 - ETA: 41:33 - loss: 0.6946 - dice_coef: 0.36 - ETA: 41:25 - loss: 0.6943 - dice_coef: 0.36 - ETA: 41:18 - loss: 0.6941 - dice_coef: 0.36 - ETA: 41:11 - loss: 0.6936 - dice_coef: 0.36 - ETA: 41:03 - loss: 0.6942 - dice_coef: 0.36 - ETA: 40:56 - loss: 0.6948 - dice_coef: 0.36 - ETA: 40:49 - loss: 0.6945 - dice_coef: 0.36 - ETA: 40:41 - loss: 0.6940 - dice_coef: 0.36 - ETA: 40:34 - loss: 0.6943 - dice_coef: 0.36 - ETA: 40:27 - loss: 0.6937 - dice_coef: 0.36 - ETA: 40:19 - loss: 0.6942 - dice_coef: 0.36 - ETA: 40:12 - loss: 0.6951 - dice_coef: 0.36 - ETA: 40:05 - loss: 0.6945 - dice_coef: 0.36 - ETA: 39:57 - loss: 0.6947 - dice_coef: 0.36 - ETA: 39:50 - loss: 0.6947 - dice_coef: 0.36 - ETA: 39:42 - loss: 0.6941 - dice_coef: 0.36 - ETA: 39:35 - loss: 0.6933 - dice_coef: 0.37 - ETA: 39:28 - loss: 0.6926 - dice_coef: 0.37 - ETA: 39:20 - loss: 0.6918 - dice_coef: 0.37 - ETA: 39:13 - loss: 0.6918 - dice_coef: 0.37 - ETA: 39:06 - loss: 0.6924 - dice_coef: 0.37 - ETA: 38:58 - loss: 0.6917 - dice_coef: 0.37 - ETA: 38:51 - loss: 0.6912 - dice_coef: 0.37 - ETA: 38:43 - loss: 0.6906 - dice_coef: 0.37 - ETA: 38:36 - loss: 0.6896 - dice_coef: 0.37 - ETA: 38:28 - loss: 0.6898 - dice_coef: 0.37 - ETA: 38:21 - loss: 0.6903 - dice_coef: 0.37 - ETA: 38:13 - loss: 0.6906 - dice_coef: 0.37 - ETA: 38:06 - loss: 0.6904 - dice_coef: 0.37 - ETA: 37:59 - loss: 0.6909 - dice_coef: 0.37 - ETA: 37:52 - loss: 0.6908 - dice_coef: 0.37 - ETA: 37:44 - loss: 0.6902 - dice_coef: 0.37 - ETA: 37:37 - loss: 0.6907 - dice_coef: 0.37 - ETA: 37:30 - loss: 0.6907 - dice_coef: 0.37 - ETA: 37:23 - loss: 0.6905 - dice_coef: 0.37 - ETA: 37:15 - loss: 0.6899 - dice_coef: 0.37 - ETA: 37:08 - loss: 0.6891 - dice_coef: 0.37 - ETA: 37:00 - loss: 0.6886 - dice_coef: 0.37 - ETA: 36:53 - loss: 0.6883 - dice_coef: 0.37 - ETA: 36:46 - loss: 0.6874 - dice_coef: 0.37 - ETA: 36:38 - loss: 0.6872 - dice_coef: 0.37 - ETA: 36:31 - loss: 0.6881 - dice_coef: 0.37 - ETA: 36:24 - loss: 0.6878 - dice_coef: 0.37 - ETA: 36:17 - loss: 0.6875 - dice_coef: 0.37 - ETA: 36:09 - loss: 0.6870 - dice_coef: 0.37 - ETA: 36:02 - loss: 0.6873 - dice_coef: 0.37 - ETA: 35:54 - loss: 0.6878 - dice_coef: 0.37 - ETA: 35:47 - loss: 0.6886 - dice_coef: 0.37 - ETA: 35:41 - loss: 0.6879 - dice_coef: 0.37 - ETA: 35:33 - loss: 0.6878 - dice_coef: 0.37 - ETA: 35:26 - loss: 0.6872 - dice_coef: 0.37 - ETA: 35:19 - loss: 0.6874 - dice_coef: 0.37 - ETA: 35:12 - loss: 0.6876 - dice_coef: 0.37 - ETA: 35:04 - loss: 0.6877 - dice_coef: 0.37 - ETA: 34:57 - loss: 0.6885 - dice_coef: 0.37 - ETA: 34:50 - loss: 0.6892 - dice_coef: 0.37 - ETA: 34:43 - loss: 0.6892 - dice_coef: 0.37 - ETA: 34:35 - loss: 0.6896 - dice_coef: 0.37 - ETA: 34:28 - loss: 0.6893 - dice_coef: 0.37 - ETA: 34:21 - loss: 0.6892 - dice_coef: 0.37 - ETA: 34:13 - loss: 0.6891 - dice_coef: 0.37 - ETA: 34:06 - loss: 0.6895 - dice_coef: 0.37 - ETA: 33:58 - loss: 0.6901 - dice_coef: 0.37 - ETA: 33:51 - loss: 0.6896 - dice_coef: 0.37 - ETA: 33:44 - loss: 0.6903 - dice_coef: 0.37 - ETA: 33:37 - loss: 0.6901 - dice_coef: 0.37 - ETA: 33:29 - loss: 0.6911 - dice_coef: 0.37 - ETA: 33:22 - loss: 0.6918 - dice_coef: 0.37 - ETA: 33:15 - loss: 0.6923 - dice_coef: 0.37 - ETA: 33:08 - loss: 0.6919 - dice_coef: 0.37 - ETA: 33:00 - loss: 0.6924 - dice_coef: 0.37 - ETA: 32:53 - loss: 0.6915 - dice_coef: 0.37 - ETA: 32:45 - loss: 0.6916 - dice_coef: 0.37 - ETA: 32:38 - loss: 0.6919 - dice_coef: 0.37 - ETA: 32:31 - loss: 0.6915 - dice_coef: 0.37 - ETA: 32:23 - loss: 0.6916 - dice_coef: 0.37 - ETA: 32:16 - loss: 0.6911 - dice_coef: 0.37 - ETA: 32:09 - loss: 0.6904 - dice_coef: 0.37 - ETA: 32:02 - loss: 0.6908 - dice_coef: 0.37 - ETA: 31:54 - loss: 0.6915 - dice_coef: 0.37 - ETA: 31:47 - loss: 0.6908 - dice_coef: 0.37 - ETA: 31:40 - loss: 0.6901 - dice_coef: 0.37 - ETA: 31:32 - loss: 0.6900 - dice_coef: 0.37 - ETA: 31:25 - loss: 0.6896 - dice_coef: 0.37 - ETA: 31:18 - loss: 0.6896 - dice_coef: 0.37 - ETA: 31:10 - loss: 0.6894 - dice_coef: 0.37 - ETA: 31:03 - loss: 0.6892 - dice_coef: 0.37 - ETA: 30:56 - loss: 0.6897 - dice_coef: 0.37 - ETA: 30:48 - loss: 0.6894 - dice_coef: 0.37 - ETA: 30:41 - loss: 0.6901 - dice_coef: 0.37 - ETA: 30:33 - loss: 0.6908 - dice_coef: 0.37 - ETA: 30:26 - loss: 0.6907 - dice_coef: 0.37 - ETA: 30:19 - loss: 0.6907 - dice_coef: 0.37 - ETA: 30:11 - loss: 0.6902 - dice_coef: 0.37 - ETA: 30:04 - loss: 0.6900 - dice_coef: 0.37 - ETA: 29:57 - loss: 0.6896 - dice_coef: 0.37 - ETA: 29:49 - loss: 0.6889 - dice_coef: 0.37 - ETA: 29:42 - loss: 0.6897 - dice_coef: 0.37 - ETA: 29:35 - loss: 0.6895 - dice_coef: 0.37 - ETA: 29:27 - loss: 0.6886 - dice_coef: 0.37 - ETA: 29:20 - loss: 0.6887 - dice_coef: 0.37 - ETA: 29:13 - loss: 0.6886 - dice_coef: 0.37 - ETA: 29:06 - loss: 0.6880 - dice_coef: 0.37 - ETA: 28:58 - loss: 0.6880 - dice_coef: 0.37 - ETA: 28:51 - loss: 0.6877 - dice_coef: 0.37 - ETA: 28:43 - loss: 0.6880 - dice_coef: 0.37 - ETA: 28:36 - loss: 0.6882 - dice_coef: 0.37 - ETA: 28:28 - loss: 0.6887 - dice_coef: 0.37 - ETA: 28:21 - loss: 0.6893 - dice_coef: 0.37 - ETA: 28:14 - loss: 0.6888 - dice_coef: 0.37 - ETA: 28:06 - loss: 0.6881 - dice_coef: 0.37 - ETA: 27:59 - loss: 0.6888 - dice_coef: 0.37 - ETA: 27:51 - loss: 0.6884 - dice_coef: 0.37 - ETA: 27:44 - loss: 0.6887 - dice_coef: 0.37 - ETA: 27:37 - loss: 0.6879 - dice_coef: 0.37 - ETA: 27:29 - loss: 0.6880 - dice_coef: 0.37 - ETA: 27:22 - loss: 0.6880 - dice_coef: 0.37 - ETA: 27:14 - loss: 0.6877 - dice_coef: 0.37 - ETA: 27:07 - loss: 0.6882 - dice_coef: 0.37 - ETA: 26:59 - loss: 0.6877 - dice_coef: 0.37 - ETA: 26:52 - loss: 0.6877 - dice_coef: 0.37 - ETA: 26:45 - loss: 0.6875 - dice_coef: 0.37 - ETA: 26:37 - loss: 0.6882 - dice_coef: 0.37 - ETA: 26:30 - loss: 0.6881 - dice_coef: 0.37 - ETA: 26:22 - loss: 0.6880 - dice_coef: 0.37 - ETA: 26:15 - loss: 0.6875 - dice_coef: 0.37 - ETA: 26:08 - loss: 0.6871 - dice_coef: 0.37 - ETA: 26:00 - loss: 0.6866 - dice_coef: 0.37 - ETA: 25:53 - loss: 0.6864 - dice_coef: 0.37 - ETA: 25:45 - loss: 0.6865 - dice_coef: 0.37 - ETA: 25:38 - loss: 0.6859 - dice_coef: 0.37 - ETA: 25:31 - loss: 0.6854 - dice_coef: 0.37 - ETA: 25:23 - loss: 0.6849 - dice_coef: 0.37 - ETA: 25:16 - loss: 0.6856 - dice_coef: 0.37 - ETA: 25:09 - loss: 0.6855 - dice_coef: 0.37 - ETA: 25:01 - loss: 0.6850 - dice_coef: 0.37 - ETA: 24:54 - loss: 0.6857 - dice_coef: 0.37 - ETA: 24:46 - loss: 0.6853 - dice_coef: 0.37 - ETA: 24:39 - loss: 0.6848 - dice_coef: 0.37 - ETA: 24:32 - loss: 0.6842 - dice_coef: 0.37 - ETA: 24:24 - loss: 0.6849 - dice_coef: 0.37 - ETA: 24:17 - loss: 0.6855 - dice_coef: 0.37 - ETA: 24:09 - loss: 0.6853 - dice_coef: 0.37 - ETA: 24:02 - loss: 0.6859 - dice_coef: 0.37 - ETA: 23:55 - loss: 0.6866 - dice_coef: 0.37 - ETA: 23:47 - loss: 0.6859 - dice_coef: 0.37 - ETA: 23:40 - loss: 0.6857 - dice_coef: 0.37 - ETA: 23:32 - loss: 0.6856 - dice_coef: 0.37 - ETA: 23:25 - loss: 0.6850 - dice_coef: 0.37 - ETA: 23:18 - loss: 0.6851 - dice_coef: 0.37 - ETA: 23:10 - loss: 0.6845 - dice_coef: 0.37 - ETA: 23:03 - loss: 0.6845 - dice_coef: 0.37 - ETA: 22:55 - loss: 0.6843 - dice_coef: 0.37 - ETA: 22:48 - loss: 0.6849 - dice_coef: 0.37 - ETA: 22:41 - loss: 0.6844 - dice_coef: 0.37 - ETA: 22:33 - loss: 0.6851 - dice_coef: 0.3783"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637/708 [=========================>....] - ETA: 22:26 - loss: 0.6851 - dice_coef: 0.37 - ETA: 22:18 - loss: 0.6845 - dice_coef: 0.37 - ETA: 22:11 - loss: 0.6846 - dice_coef: 0.37 - ETA: 22:04 - loss: 0.6843 - dice_coef: 0.37 - ETA: 21:56 - loss: 0.6840 - dice_coef: 0.37 - ETA: 21:49 - loss: 0.6842 - dice_coef: 0.37 - ETA: 21:42 - loss: 0.6843 - dice_coef: 0.37 - ETA: 21:34 - loss: 0.6846 - dice_coef: 0.37 - ETA: 21:27 - loss: 0.6853 - dice_coef: 0.37 - ETA: 21:20 - loss: 0.6850 - dice_coef: 0.37 - ETA: 21:13 - loss: 0.6848 - dice_coef: 0.37 - ETA: 21:06 - loss: 0.6844 - dice_coef: 0.37 - ETA: 20:58 - loss: 0.6850 - dice_coef: 0.37 - ETA: 20:51 - loss: 0.6845 - dice_coef: 0.37 - ETA: 20:44 - loss: 0.6839 - dice_coef: 0.37 - ETA: 20:37 - loss: 0.6831 - dice_coef: 0.38 - ETA: 20:30 - loss: 0.6833 - dice_coef: 0.38 - ETA: 20:23 - loss: 0.6825 - dice_coef: 0.38 - ETA: 20:16 - loss: 0.6825 - dice_coef: 0.38 - ETA: 20:08 - loss: 0.6825 - dice_coef: 0.38 - ETA: 20:01 - loss: 0.6821 - dice_coef: 0.38 - ETA: 19:54 - loss: 0.6818 - dice_coef: 0.38 - ETA: 19:46 - loss: 0.6814 - dice_coef: 0.38 - ETA: 19:39 - loss: 0.6809 - dice_coef: 0.38 - ETA: 19:31 - loss: 0.6805 - dice_coef: 0.38 - ETA: 19:24 - loss: 0.6797 - dice_coef: 0.38 - ETA: 19:17 - loss: 0.6799 - dice_coef: 0.38 - ETA: 19:09 - loss: 0.6802 - dice_coef: 0.38 - ETA: 19:02 - loss: 0.6802 - dice_coef: 0.38 - ETA: 18:54 - loss: 0.6800 - dice_coef: 0.38 - ETA: 18:47 - loss: 0.6799 - dice_coef: 0.38 - ETA: 18:40 - loss: 0.6792 - dice_coef: 0.38 - ETA: 18:32 - loss: 0.6791 - dice_coef: 0.38 - ETA: 18:25 - loss: 0.6793 - dice_coef: 0.38 - ETA: 18:17 - loss: 0.6798 - dice_coef: 0.38 - ETA: 18:10 - loss: 0.6802 - dice_coef: 0.38 - ETA: 18:02 - loss: 0.6803 - dice_coef: 0.38 - ETA: 17:55 - loss: 0.6798 - dice_coef: 0.38 - ETA: 17:48 - loss: 0.6793 - dice_coef: 0.38 - ETA: 17:40 - loss: 0.6794 - dice_coef: 0.38 - ETA: 17:33 - loss: 0.6797 - dice_coef: 0.38 - ETA: 17:26 - loss: 0.6800 - dice_coef: 0.38 - ETA: 17:18 - loss: 0.6801 - dice_coef: 0.38 - ETA: 17:11 - loss: 0.6796 - dice_coef: 0.38 - ETA: 17:03 - loss: 0.6796 - dice_coef: 0.38 - ETA: 16:56 - loss: 0.6794 - dice_coef: 0.38 - ETA: 16:48 - loss: 0.6787 - dice_coef: 0.38 - ETA: 16:41 - loss: 0.6787 - dice_coef: 0.38 - ETA: 16:34 - loss: 0.6791 - dice_coef: 0.38 - ETA: 16:26 - loss: 0.6788 - dice_coef: 0.38 - ETA: 16:19 - loss: 0.6786 - dice_coef: 0.38 - ETA: 16:11 - loss: 0.6791 - dice_coef: 0.38 - ETA: 16:04 - loss: 0.6784 - dice_coef: 0.38 - ETA: 15:57 - loss: 0.6786 - dice_coef: 0.38 - ETA: 15:49 - loss: 0.6790 - dice_coef: 0.38 - ETA: 15:42 - loss: 0.6790 - dice_coef: 0.38 - ETA: 15:34 - loss: 0.6792 - dice_coef: 0.38 - ETA: 15:27 - loss: 0.6799 - dice_coef: 0.38 - ETA: 15:20 - loss: 0.6799 - dice_coef: 0.38 - ETA: 15:12 - loss: 0.6805 - dice_coef: 0.38 - ETA: 15:05 - loss: 0.6811 - dice_coef: 0.38 - ETA: 14:57 - loss: 0.6805 - dice_coef: 0.38 - ETA: 14:50 - loss: 0.6809 - dice_coef: 0.38 - ETA: 14:43 - loss: 0.6809 - dice_coef: 0.38 - ETA: 14:35 - loss: 0.6809 - dice_coef: 0.38 - ETA: 14:28 - loss: 0.6804 - dice_coef: 0.38 - ETA: 14:21 - loss: 0.6808 - dice_coef: 0.38 - ETA: 14:13 - loss: 0.6803 - dice_coef: 0.38 - ETA: 14:06 - loss: 0.6802 - dice_coef: 0.38 - ETA: 13:58 - loss: 0.6801 - dice_coef: 0.38 - ETA: 13:51 - loss: 0.6801 - dice_coef: 0.38 - ETA: 13:44 - loss: 0.6807 - dice_coef: 0.38 - ETA: 13:36 - loss: 0.6812 - dice_coef: 0.38 - ETA: 13:29 - loss: 0.6814 - dice_coef: 0.38 - ETA: 13:21 - loss: 0.6817 - dice_coef: 0.38 - ETA: 13:14 - loss: 0.6824 - dice_coef: 0.38 - ETA: 13:07 - loss: 0.6819 - dice_coef: 0.38 - ETA: 12:59 - loss: 0.6822 - dice_coef: 0.38 - ETA: 12:52 - loss: 0.6824 - dice_coef: 0.38 - ETA: 12:44 - loss: 0.6822 - dice_coef: 0.38 - ETA: 12:37 - loss: 0.6822 - dice_coef: 0.38 - ETA: 12:30 - loss: 0.6827 - dice_coef: 0.38 - ETA: 12:22 - loss: 0.6828 - dice_coef: 0.37 - ETA: 12:15 - loss: 0.6832 - dice_coef: 0.37 - ETA: 12:08 - loss: 0.6830 - dice_coef: 0.37 - ETA: 12:00 - loss: 0.6835 - dice_coef: 0.37 - ETA: 11:53 - loss: 0.6838 - dice_coef: 0.37 - ETA: 11:46 - loss: 0.6843 - dice_coef: 0.37 - ETA: 11:38 - loss: 0.6847 - dice_coef: 0.37 - ETA: 11:31 - loss: 0.6844 - dice_coef: 0.37 - ETA: 11:23 - loss: 0.6845 - dice_coef: 0.37 - ETA: 11:16 - loss: 0.6849 - dice_coef: 0.37 - ETA: 11:09 - loss: 0.6845 - dice_coef: 0.37 - ETA: 11:01 - loss: 0.6851 - dice_coef: 0.37 - ETA: 10:54 - loss: 0.6852 - dice_coef: 0.37 - ETA: 10:47 - loss: 0.6850 - dice_coef: 0.37 - ETA: 10:39 - loss: 0.6846 - dice_coef: 0.37 - ETA: 10:32 - loss: 0.6848 - dice_coef: 0.37 - ETA: 10:24 - loss: 0.6847 - dice_coef: 0.37 - ETA: 10:17 - loss: 0.6849 - dice_coef: 0.37 - ETA: 10:10 - loss: 0.6854 - dice_coef: 0.37 - ETA: 10:03 - loss: 0.6858 - dice_coef: 0.37 - ETA: 9:55 - loss: 0.6855 - dice_coef: 0.3782 - ETA: 9:48 - loss: 0.6859 - dice_coef: 0.377 - ETA: 9:40 - loss: 0.6859 - dice_coef: 0.377 - ETA: 9:33 - loss: 0.6864 - dice_coef: 0.377 - ETA: 9:26 - loss: 0.6858 - dice_coef: 0.377 - ETA: 9:18 - loss: 0.6860 - dice_coef: 0.377 - ETA: 9:11 - loss: 0.6859 - dice_coef: 0.377 - ETA: 9:04 - loss: 0.6860 - dice_coef: 0.377 - ETA: 8:56 - loss: 0.6861 - dice_coef: 0.377 - ETA: 8:49 - loss: 0.6856 - dice_coef: 0.377 - ETA: 8:41 - loss: 0.6855 - dice_coef: 0.3779"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'D:/data/Kaggle/Severstal/model_2.1.h5', \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[checkpoint],\n",
    "    use_multiprocessing=False,\n",
    "    workers=1,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tta_wrapper import tta_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 229 layers into a model with 28 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-fc6fa429024a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/data/Kaggle/Severstal/modelB2tta.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"load_model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtta_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtta_segmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_flip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_shift\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmul\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1164\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[1;32m-> 1166\u001b[1;33m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Selvaria\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers, reshape)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                          \u001b[1;34m'containing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                          \u001b[1;34m' layers into a model with '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to load a weight file containing 229 layers into a model with 28 layers."
     ]
    }
   ],
   "source": [
    "#model.load_weights('D:/data/Kaggle/Severstal/modelB2tta.h5')\n",
    "#print(\"load_model\")\n",
    "test_df = []\n",
    "tta_model = tta_segmentation(model, h_flip=True, h_shift=(-10, 10),mul =[0.95], merge='mean')\n",
    "\n",
    "for i in range(0, filtered_test_imgs.shape[0], 300):\n",
    "    batch_idx = list(\n",
    "        range(i, min(filtered_test_imgs.shape[0], i + 300))\n",
    "    )\n",
    "    \n",
    "    test_generator = DataGenerator(\n",
    "        batch_idx,\n",
    "        df=filtered_test_imgs,\n",
    "        shuffle=False,\n",
    "        mode='predict',\n",
    "        base_path='D:/data/Kaggle/Severstal/test_imgs',\n",
    "        target_df=filtered_sub_df,\n",
    "        batch_size=1,\n",
    "        n_classes=4,aug = None\n",
    "    )\n",
    "    \n",
    "    batch_pred_masks = tta_model.predict_generator(\n",
    "        test_generator, \n",
    "        workers=1,\n",
    "        verbose=1,\n",
    "        use_multiprocessing=False\n",
    "    )\n",
    "    \n",
    "    for j, b in tqdm(enumerate(batch_idx)):\n",
    "        filename = filtered_test_imgs['ImageId'].iloc[b]\n",
    "        image_df = filtered_sub_df[filtered_sub_df['ImageId'] == filename].copy()\n",
    "        \n",
    "#         pred_masks = batch_pred_masks[j, ].round().astype(int)\n",
    "        pred_masks = batch_pred_masks[j, ]\n",
    "        pred_masks[pred_masks<0.95]=0\n",
    "        pred_masks[pred_masks>0.5]=1\n",
    "        pred_rles = build_rles(pred_masks.astype(int))\n",
    "        \n",
    "        image_df['EncodedPixels'] = pred_rles\n",
    "        test_df.append(image_df)\n",
    "        \n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
